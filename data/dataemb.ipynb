{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed9724b",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1446\n",
      "Selected sample_ids:\n",
      "['s_000413', 's_000316', 's_000554', 's_000065', 's_001380', 's_000967', 's_000175', 's_000836', 's_000651', 's_000231']\n",
      "\n",
      "Demo dataset built: 10 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \"/Users/lixiaokang/daily/data/train_samples\"\n",
    "\n",
    "META_DIR = os.path.join(ROOT, \"meta\")\n",
    "IMG_DIR = os.path.join(ROOT, \"images\")\n",
    "TAB_DIR = os.path.join(ROOT, \"tabular\")\n",
    "TS_DIR = os.path.join(ROOT, \"timeseries_merged\")\n",
    "\n",
    "SAMPLE_INDEX_PATH = os.path.join(META_DIR, \"sample_index.json\")\n",
    "\n",
    "with open(SAMPLE_INDEX_PATH, \"r\") as f:\n",
    "    sample_index = json.load(f)\n",
    "\n",
    "df_index = pd.DataFrame(sample_index)\n",
    "\n",
    "print(\"Total samples:\", len(df_index))\n",
    "\n",
    "df_demo = df_index.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Selected sample_ids:\")\n",
    "print(df_demo[\"sample_id\"].tolist())\n",
    "\n",
    "TAB_FILES = [\n",
    "    \"admissions.csv\",\n",
    "    \"demographics.csv\",\n",
    "    \"icustays.csv\",\n",
    "]\n",
    "\n",
    "tabular_tables = {}\n",
    "for fname in TAB_FILES:\n",
    "    path = os.path.join(TAB_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        tabular_tables[fname.replace(\".csv\", \"\")] = pd.read_csv(path)\n",
    "\n",
    "TS_FILES = [\n",
    "    \"labevents.csv\",\n",
    "    \"chartevents.csv\",\n",
    "]\n",
    "\n",
    "timeseries_tables = {}\n",
    "for fname in TS_FILES:\n",
    "    path = os.path.join(TS_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        if \"charttime\" in df.columns:\n",
    "            df[\"charttime\"] = pd.to_datetime(df[\"charttime\"])\n",
    "        timeseries_tables[fname.replace(\".csv\", \"\")] = df\n",
    "\n",
    "demo_samples = []\n",
    "\n",
    "for _, row in df_demo.iterrows():\n",
    "    sample_id = row[\"sample_id\"]\n",
    "    subject_id = row[\"subject_id\"]\n",
    "    cxr_time = pd.to_datetime(row[\"cxr_time\"])\n",
    "\n",
    "    sample = {\n",
    "        \"sample_id\": sample_id,\n",
    "        \"subject_id\": subject_id,\n",
    "        \"meta\": row.to_dict(),\n",
    "    }\n",
    "\n",
    "    sample_id = row[\"sample_id\"]\n",
    "    img_path = os.path.join(IMG_DIR, f\"{sample_id}.npy\")\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        sample[\"image\"] = np.load(img_path)\n",
    "    else:\n",
    "        print(f\"[WARN] image not found for {sample_id}\")\n",
    "        sample[\"image\"] = None\n",
    "\n",
    "    tab_data = {}\n",
    "    for name, df in tabular_tables.items():\n",
    "        if \"subject_id\" in df.columns:\n",
    "            tab_data[name] = df[df[\"subject_id\"] == subject_id]\n",
    "    sample[\"tabular\"] = tab_data\n",
    "\n",
    "    ts_data = {}\n",
    "    for name, df in timeseries_tables.items():\n",
    "        if \"subject_id\" in df.columns and \"charttime\" in df.columns:\n",
    "            ts_data[name] = df[\n",
    "                (df[\"subject_id\"] == subject_id) &\n",
    "                (df[\"charttime\"] <= cxr_time) &\n",
    "                (df[\"charttime\"] >= cxr_time - pd.Timedelta(hours=48))\n",
    "            ]\n",
    "    sample[\"timeseries\"] = ts_data\n",
    "\n",
    "    sample[\"label_any\"] = row.get(\"label_any\", None)\n",
    "    sample[\"label_multi\"] = row.get(\"label_multi\", None)\n",
    "\n",
    "    demo_samples.append(sample)\n",
    "\n",
    "print(f\"\\nDemo dataset built: {len(demo_samples)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f5c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1446\n",
      "Selected sample_ids: ['s_000413', 's_000316', 's_000554', 's_000065', 's_001380', 's_000967', 's_000175', 's_000836', 's_000651', 's_000231']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86173\\AppData\\Local\\Temp\\ipykernel_48216\\3477631609.py:39: DtypeWarning: Columns (7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tabular_tables[fname[:-4]] = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo dataset built: 10 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# paths\n",
    "ROOT = \"E:/NUS/data/perdata/train_text_samples\"\n",
    "\n",
    "META_DIR = os.path.join(ROOT, \"meta\")\n",
    "IMG_DIR = os.path.join(ROOT, \"images\")\n",
    "TAB_DIR = os.path.join(ROOT, \"tabular\")\n",
    "TS_DIR = os.path.join(ROOT, \"timeseries_merged\")\n",
    "TEXT_PATH = os.path.join(ROOT, \"text.csv\")\n",
    "\n",
    "SAMPLE_INDEX_PATH = os.path.join(META_DIR, \"sample_index.json\")\n",
    "\n",
    "# load sample index\n",
    "with open(SAMPLE_INDEX_PATH, \"r\") as f:\n",
    "    sample_index = json.load(f)\n",
    "\n",
    "df_index = pd.DataFrame(sample_index)\n",
    "print(\"Total samples:\", len(df_index))\n",
    "\n",
    "\n",
    "# random select demo samples\n",
    "df_demo = df_index.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "print(\"Selected sample_ids:\", df_demo[\"sample_id\"].tolist())\n",
    "\n",
    "# load tabular tables (once)\n",
    "# TAB_FILES = [\"admissions.csv\", \"demographics.csv\", \"icustays.csv\"]\n",
    "tabular_tables = {}\n",
    "\n",
    "TAB_FILES = [f for f in os.listdir(TAB_DIR) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "for fname in TAB_FILES:\n",
    "    path = os.path.join(TAB_DIR, fname)\n",
    "    try:\n",
    "        tabular_tables[fname[:-4]] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to read {fname}: {e}\")\n",
    "\n",
    "\n",
    "# load & preprocess text table (once)\n",
    "if os.path.exists(TEXT_PATH):\n",
    "    df_text = pd.read_csv(TEXT_PATH)\n",
    "\n",
    "    if \"cxr_time\" in df_text.columns:\n",
    "        df_text[\"cxr_time\"] = pd.to_datetime(df_text[\"cxr_time\"], errors=\"coerce\")\n",
    "\n",
    "    df_text[\"text\"] = df_text[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "    if \"dicom_id\" in df_text.columns:\n",
    "        text_map_dicom = (\n",
    "            df_text.groupby(\"dicom_id\")[\"text\"]\n",
    "            .apply(lambda s: \"\\n\".join([t.strip() for t in s if t.strip()]))\n",
    "            .to_dict()\n",
    "        )\n",
    "    else:\n",
    "        text_map_dicom = {}\n",
    "\n",
    "else:\n",
    "    print(\"[WARN] text.csv not found\")\n",
    "    df_text = None\n",
    "    text_map_dicom = {}\n",
    "\n",
    "\n",
    "# load timeseries tables (once)\n",
    "timeseries_tables = {}\n",
    "\n",
    "TS_FILES = [f for f in os.listdir(TS_DIR) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "for fname in TS_FILES:\n",
    "    path = os.path.join(TS_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        if \"charttime\" in df.columns:\n",
    "            df[\"charttime\"] = pd.to_datetime(df[\"charttime\"], errors=\"coerce\")\n",
    "        timeseries_tables[fname[:-4]] = df  # remove \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "# build demo samples (CXR-level)\n",
    "\n",
    "demo_samples = []\n",
    "\n",
    "for _, row in df_demo.iterrows():\n",
    "    sample_id = row[\"sample_id\"]\n",
    "    subject_id = row[\"subject_id\"]\n",
    "    cxr_time = pd.to_datetime(row[\"cxr_time\"], errors=\"coerce\")\n",
    "    dicom_id = row.get(\"dicom_id\", None)\n",
    "\n",
    "    sample = {\n",
    "        \"sample_id\": sample_id,\n",
    "        \"subject_id\": subject_id,\n",
    "        \"dicom_id\": dicom_id,\n",
    "        \"cxr_time\": cxr_time,\n",
    "        \"meta\": row.to_dict(),\n",
    "    }\n",
    "\n",
    "    # image (1 CXR = 1 image)\n",
    "    img_path = os.path.join(IMG_DIR, f\"{sample_id}.npy\")\n",
    "    sample[\"image\"] = np.load(img_path) if os.path.exists(img_path) else None\n",
    "\n",
    "    # text\n",
    "    text = \"\"\n",
    "\n",
    "    if dicom_id is not None and dicom_id in text_map_dicom:\n",
    "        text = text_map_dicom[dicom_id]\n",
    "\n",
    "    elif df_text is not None:\n",
    "        rows = df_text[\n",
    "            (df_text[\"subject_id\"] == subject_id) &\n",
    "            (df_text[\"cxr_time\"].notna()) &\n",
    "            (abs(df_text[\"cxr_time\"] - cxr_time) <= pd.Timedelta(minutes=5))\n",
    "        ]\n",
    "        if len(rows) > 0:\n",
    "            text = \"\\n\".join(\n",
    "                [t.strip() for t in rows[\"text\"].tolist() if t.strip()]\n",
    "            )\n",
    "\n",
    "    sample[\"text\"] = text  \n",
    "\n",
    "    # tabular (patient-level snapshot)\n",
    "    tab_data = {}\n",
    "    for name, df in tabular_tables.items():\n",
    "        if \"subject_id\" in df.columns:\n",
    "            tab_data[name] = df[df[\"subject_id\"] == subject_id]\n",
    "    sample[\"tabular\"] = tab_data\n",
    "\n",
    "    # timeseries (48h before CXR)\n",
    "    ts_data = {}\n",
    "    for name, df in timeseries_tables.items():\n",
    "        if \"subject_id\" in df.columns and \"charttime\" in df.columns and pd.notna(cxr_time):\n",
    "            ts_data[name] = df[\n",
    "                (df[\"subject_id\"] == subject_id) &\n",
    "                (df[\"charttime\"] <= cxr_time) &\n",
    "                (df[\"charttime\"] >= cxr_time - pd.Timedelta(hours=48))\n",
    "            ]\n",
    "    sample[\"timeseries\"] = ts_data\n",
    "\n",
    "    # labels\n",
    "    sample[\"label_any\"] = row.get(\"label_any\", None)\n",
    "    sample[\"label_multi\"] = row.get(\"label_multi\", None)\n",
    "\n",
    "    demo_samples.append(sample)\n",
    "\n",
    "print(f\"\\nDemo dataset built: {len(demo_samples)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce28e5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chartevents] scanned chunks=0, rows=602,046\n",
      "[DONE] chartevents: rows=602,046, size=135.94MB, miss_sid=0.0000%\n",
      "[labevents] scanned chunks=0, rows=51,765\n",
      "[DONE] labevents: rows=51,765, size=10.60MB, miss_sid=0.0000%\n",
      "[inputevents] scanned chunks=0, rows=313,095\n",
      "[DONE] inputevents: rows=313,095, size=116.46MB, miss_sid=0.0000%\n",
      "[outputevents] scanned chunks=0, rows=9,228\n",
      "[DONE] outputevents: rows=9,228, size=1.72MB, miss_sid=0.0000%\n",
      "[procedureevents] scanned chunks=0, rows=18,214\n",
      "[DONE] procedureevents: rows=18,214, size=5.89MB, miss_sid=0.0000%\n",
      "\n",
      "===== SUMMARY =====\n",
      "          table  size_mb  rows_total  rows_valid_subject  subject_missing_rate  b128_mean_rows  b128_max_rows  b128_p95_rows  b256_mean_rows  b256_max_rows  b256_p95_rows  b512_mean_rows  b512_max_rows  b512_p95_rows  b1024_mean_rows  b1024_max_rows  b1024_p95_rows\n",
      "    chartevents   135.94      602046              602046                   0.0            4703          66014          34284            2351          66014          15513            1175          66014           3310              587           66014               0\n",
      "    inputevents   116.46      313095              313095                   0.0            2446          41238          17696            1223          41238           6654             611          41220            752              305           41220              84\n",
      "      labevents    10.60       51765               51765                   0.0             404           7183           2929             202           7183           1355             101           6984            272               50            6984               0\n",
      "procedureevents     5.89       18214               18214                   0.0             142           2132           1030              71           2132            350              35           2132             91               17            2132              17\n",
      "   outputevents     1.72        9228                9228                   0.0              72           1104            501              36           1104            213              18           1104             50                9            1104               0\n",
      "\n",
      "Saved: timeseries_bucket_planning.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import TS_CFG\n",
    "\n",
    "def analyze_timeseries_tables(\n",
    "    TS_DIR: str,\n",
    "    tables=None,\n",
    "    chunksize: int = 1_000_000,\n",
    "    candidates=(128, 256, 512, 1024),\n",
    "):\n",
    "    \"\"\"\n",
    "    逐表按 chunk 扫描：\n",
    "      - 总行数\n",
    "      - subject_id 缺失率\n",
    "      - 对每个候选 num_buckets，统计 bucket 行数分布（max/mean）\n",
    "    不做时间列解析，速度快，内存稳。\n",
    "    \"\"\"\n",
    "    if tables is None:\n",
    "        tables = list(TS_CFG.keys())\n",
    "\n",
    "    report = []\n",
    "\n",
    "    for table in tables:\n",
    "        path = os.path.join(TS_DIR, f\"{table}.csv\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[MISS] {table}.csv\")\n",
    "            continue\n",
    "\n",
    "        cfg = TS_CFG.get(table, {})\n",
    "        # 统计时只读 subject_id 就够了（速度最快）\n",
    "        usecols = cfg.get(\"usecols\")\n",
    "        if usecols is None or \"subject_id\" not in usecols:\n",
    "            usecols = [\"subject_id\"]\n",
    "        else:\n",
    "            usecols = [\"subject_id\"]  # 统计阶段只取这列即可\n",
    "\n",
    "        # 为了稳，dtype 用 object/float 都行；这里用 object 再转 numeric\n",
    "        reader = pd.read_csv(\n",
    "            path,\n",
    "            usecols=usecols,\n",
    "            chunksize=chunksize,\n",
    "            low_memory=False,\n",
    "        )\n",
    "\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "\n",
    "        total_rows = 0\n",
    "        missing_sid = 0\n",
    "\n",
    "        # 为每个 candidate 准备一个 bincount 数组\n",
    "        # 注意：num_buckets 越大，数组越大，但 1024 也就 1024 个 int64，很小\n",
    "        bucket_counts = {B: np.zeros(B, dtype=np.int64) for B in candidates}\n",
    "\n",
    "        for chunk_id, df in enumerate(reader):\n",
    "            total_rows += len(df)\n",
    "\n",
    "            sid = pd.to_numeric(df[\"subject_id\"], errors=\"coerce\")\n",
    "            missing_sid += sid.isna().sum()\n",
    "\n",
    "            sid = sid.dropna().astype(np.int64).to_numpy()\n",
    "\n",
    "            # 对每个候选桶数，统计该 chunk 的桶分布并累加\n",
    "            for B in candidates:\n",
    "                b = sid % B\n",
    "                bc = np.bincount(b, minlength=B)\n",
    "                bucket_counts[B] += bc\n",
    "\n",
    "            if chunk_id % 10 == 0:\n",
    "                print(f\"[{table}] scanned chunks={chunk_id}, rows={total_rows:,}\")\n",
    "\n",
    "        valid_rows = total_rows - missing_sid\n",
    "        miss_rate = (missing_sid / total_rows) if total_rows > 0 else 0.0\n",
    "\n",
    "        row = {\n",
    "            \"table\": table,\n",
    "            \"size_mb\": round(size_mb, 2),\n",
    "            \"rows_total\": int(total_rows),\n",
    "            \"rows_valid_subject\": int(valid_rows),\n",
    "            \"subject_missing_rate\": round(miss_rate, 6),\n",
    "        }\n",
    "\n",
    "        # 汇总每个候选桶数的 max/mean\n",
    "        for B in candidates:\n",
    "            counts = bucket_counts[B]\n",
    "            row[f\"b{B}_mean_rows\"] = int(counts.mean())\n",
    "            row[f\"b{B}_max_rows\"]  = int(counts.max())\n",
    "            # 可选：95分位数看看“通常最坏多大”\n",
    "            row[f\"b{B}_p95_rows\"]  = int(np.quantile(counts, 0.95))\n",
    "\n",
    "        report.append(row)\n",
    "\n",
    "        print(f\"[DONE] {table}: rows={total_rows:,}, size={size_mb:.2f}MB, miss_sid={miss_rate:.4%}\")\n",
    "\n",
    "    return pd.DataFrame(report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TS_DIR = r\"E:/NUS/data/perdata/train_text_samples/timeseries_merged\"\n",
    "    df = analyze_timeseries_tables(\n",
    "        TS_DIR=TS_DIR,\n",
    "        tables=list(TS_CFG.keys()),\n",
    "        chunksize=1_000_000,\n",
    "        candidates=(128, 256, 512, 1024),\n",
    "    )\n",
    "    print(\"\\n===== SUMMARY =====\")\n",
    "    print(df.sort_values(\"rows_total\", ascending=False).to_string(index=False))\n",
    "    df.to_csv(\"timeseries_bucket_planning.csv\", index=False)\n",
    "    print(\"\\nSaved: timeseries_bucket_planning.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74eb1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 224, 224])\n",
      "tensor([True, True])\n",
      "2 dict_keys(['admissions', 'demographics', 'diagnoses_icd', 'icustays', 'prescriptions', 'procedures_icd', 'transfers'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Any, Optional, Callable, Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    One item = one CXR event (image-level).\n",
    "\n",
    "    Expected sample dict keys (from your demo_samples):\n",
    "      - image: np.ndarray (H,W) or (1,H,W) or None\n",
    "      - text: str (can be empty)\n",
    "      - tabular: Dict[str, pd.DataFrame]\n",
    "      - timeseries: Dict[str, pd.DataFrame]\n",
    "      - label_any: int/float/bool or None\n",
    "      - label_multi: list/np.ndarray or None\n",
    "      - meta fields: sample_id, subject_id, dicom_id, cxr_time (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples: List[Dict[str, Any]],\n",
    "        image_transform: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "        return_meta: bool = True,\n",
    "        image_normalize_255: bool = True,  # if uint8 0~255, normalize to 0~1\n",
    "    ):\n",
    "        self.samples = samples\n",
    "        self.image_transform = image_transform\n",
    "        self.return_meta = return_meta\n",
    "        self.image_normalize_255 = image_normalize_255\n",
    "\n",
    "        # infer multi-label dimension (optional but helpful)\n",
    "        self.num_labels = None\n",
    "        for s in samples:\n",
    "            lm = s.get(\"label_multi\", None)\n",
    "            if lm is not None:\n",
    "                try:\n",
    "                    self.num_labels = int(len(lm))\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _to_image_tensor(self, image: Any) -> Optional[torch.Tensor]:\n",
    "        if image is None:\n",
    "            return None\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            img = image.float()\n",
    "        else:\n",
    "            img = torch.from_numpy(np.asarray(image)).float()\n",
    "\n",
    "        # shape to (1,H,W)\n",
    "        if img.ndim == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "        elif img.ndim == 3 and img.shape[0] != 1:\n",
    "            # if (H,W,C) or (C,H,W) unexpected, you can adjust here\n",
    "            # for safety: if last dim is 1, convert to (1,H,W)\n",
    "            if img.shape[-1] == 1:\n",
    "                img = img.permute(2, 0, 1)\n",
    "            else:\n",
    "                # fallback: take first channel\n",
    "                img = img[0:1, ...]\n",
    "        elif img.ndim != 3:\n",
    "            # unexpected\n",
    "            return None\n",
    "\n",
    "        # normalize if looks like 0~255\n",
    "        if self.image_normalize_255:\n",
    "            if img.max() > 1.5:\n",
    "                img = img / 255.0\n",
    "\n",
    "        if self.image_transform is not None:\n",
    "            img = self.image_transform(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        s = self.samples[idx]\n",
    "\n",
    "        # image\n",
    "        image = self._to_image_tensor(s.get(\"image\", None))\n",
    "\n",
    "        # text\n",
    "        text = s.get(\"text\", \"\")\n",
    "        if text is None:\n",
    "            text = \"\"\n",
    "\n",
    "        # tabular & timeseries: keep as-is (DataFrame dicts)\n",
    "        tabular = s.get(\"tabular\", {}) or {}\n",
    "        timeseries = s.get(\"timeseries\", {}) or {}\n",
    "\n",
    "        # labels -> tensor-friendly\n",
    "        label_any = s.get(\"label_any\", None)\n",
    "        if label_any is None:\n",
    "            label_any_t = None\n",
    "        else:\n",
    "            label_any_t = torch.tensor(float(label_any), dtype=torch.float32)\n",
    "\n",
    "        label_multi = s.get(\"label_multi\", None)\n",
    "        if label_multi is None:\n",
    "            label_multi_t = None\n",
    "        else:\n",
    "            label_multi_arr = np.asarray(label_multi, dtype=np.float32)\n",
    "            label_multi_t = torch.from_numpy(label_multi_arr)\n",
    "\n",
    "        item = {\n",
    "            \"image\": image,            # torch.Tensor or None\n",
    "            \"text\": text,              # str\n",
    "            \"tabular\": tabular,        # Dict[str, DataFrame]\n",
    "            \"timeseries\": timeseries,  # Dict[str, DataFrame]\n",
    "            \"label_any\": label_any_t,  # torch.Tensor or None\n",
    "            \"label_multi\": label_multi_t,  # torch.Tensor(C,) or None\n",
    "        }\n",
    "\n",
    "        if self.return_meta:\n",
    "            item.update({\n",
    "                \"sample_id\": s.get(\"sample_id\"),\n",
    "                \"subject_id\": s.get(\"subject_id\"),\n",
    "                \"dicom_id\": s.get(\"dicom_id\"),\n",
    "                \"cxr_time\": s.get(\"cxr_time\"),\n",
    "            })\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "def collate_multimodal(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Custom collate that:\n",
    "      - stacks images into (B,1,H,W) with zero-fill if None, plus image_mask\n",
    "      - keeps text as List[str]\n",
    "      - keeps tabular/timeseries as List[Dict[str, DataFrame]]\n",
    "      - stacks labels if present, else returns None\n",
    "      - keeps meta as List\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # ---- images ----\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    has_img = [img is not None for img in images]\n",
    "    image_mask = torch.tensor(has_img, dtype=torch.bool)\n",
    "\n",
    "    if any(has_img):\n",
    "        # find reference shape\n",
    "        ref = next(img for img in images if img is not None)\n",
    "        B, C, H, W = len(images), ref.shape[0], ref.shape[1], ref.shape[2]\n",
    "        img_batch = torch.zeros((B, C, H, W), dtype=ref.dtype)\n",
    "        for i, img in enumerate(images):\n",
    "            if img is not None:\n",
    "                # if shapes mismatch, you could add resize/pad here\n",
    "                img_batch[i] = img\n",
    "        out[\"image\"] = img_batch\n",
    "    else:\n",
    "        out[\"image\"] = None\n",
    "\n",
    "    out[\"image_mask\"] = image_mask  # (B,)\n",
    "\n",
    "    # ---- text ----\n",
    "    out[\"text\"] = [b.get(\"text\", \"\") or \"\" for b in batch]\n",
    "\n",
    "    # ---- tabular/timeseries (keep list-of-dicts of DataFrames) ----\n",
    "    out[\"tabular\"] = [b.get(\"tabular\", {}) or {} for b in batch]\n",
    "    out[\"timeseries\"] = [b.get(\"timeseries\", {}) or {} for b in batch]\n",
    "\n",
    "    # ---- labels ----\n",
    "    la = [b.get(\"label_any\", None) for b in batch]\n",
    "    if all(x is not None for x in la):\n",
    "        out[\"label_any\"] = torch.stack(la, dim=0)  # (B,)\n",
    "    else:\n",
    "        out[\"label_any\"] = None\n",
    "\n",
    "    lm = [b.get(\"label_multi\", None) for b in batch]\n",
    "    if all(x is not None for x in lm):\n",
    "        out[\"label_multi\"] = torch.stack(lm, dim=0)  # (B,C)\n",
    "    else:\n",
    "        out[\"label_multi\"] = None\n",
    "\n",
    "    # ---- meta (optional keys) ----\n",
    "    for k in [\"sample_id\", \"subject_id\", \"dicom_id\", \"cxr_time\"]:\n",
    "        if k in batch[0]:\n",
    "            out[k] = [b.get(k) for b in batch]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===== Usage example =====\n",
    "dataset = MultiModalDataset(demo_samples, image_transform=None, return_meta=True)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_multimodal)\n",
    "batch = next(iter(loader))\n",
    "print(batch[\"image\"].shape if batch[\"image\"] is not None else None)\n",
    "print(batch[\"image_mask\"])\n",
    "print(len(batch[\"tabular\"]), batch[\"tabular\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b349b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id: s_000413\n",
      "dicom_id: 4a985010-1425fdc1-f649f309-8b269e24-6d85c760\n",
      "image shape: torch.Size([1, 224, 224])\n",
      "text preview: \n",
      "tabular keys: dict_keys(['admissions', 'demographics', 'diagnoses_icd', 'icustays', 'prescriptions', 'procedures_icd', 'transfers'])\n",
      "timeseries keys: dict_keys(['chartevents', 'labevents', 'outputevents'])\n",
      "label_any: 1\n",
      "label_multi: [1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = MultiModalDataset(\n",
    "    demo_samples,\n",
    "    image_transform=None,   \n",
    "    return_meta=True,\n",
    ")\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"sample_id:\", sample[\"sample_id\"])\n",
    "print(\"dicom_id:\", sample[\"dicom_id\"])\n",
    "print(\"image shape:\", None if sample[\"image\"] is None else sample[\"image\"].shape)\n",
    "print(\"text preview:\", sample[\"text\"][:300])\n",
    "print(\"tabular keys:\", sample[\"tabular\"].keys())\n",
    "print(\"timeseries keys:\", sample[\"timeseries\"].keys())\n",
    "print(\"label_any:\", sample[\"label_any\"])\n",
    "print(\"label_multi:\", sample[\"label_multi\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44653c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 10\n",
      "Samples with text: 6\n",
      "Samples without text: 4\n",
      "Text coverage: 60.00%\n"
     ]
    }
   ],
   "source": [
    "num_total = len(dataset)\n",
    "\n",
    "num_non_empty = 0\n",
    "num_empty = 0\n",
    "\n",
    "for s in dataset:\n",
    "    txt = s[\"text\"]\n",
    "    if txt is not None and txt.strip() != \"\":\n",
    "        num_non_empty += 1\n",
    "    else:\n",
    "        num_empty += 1\n",
    "\n",
    "print(f\"Total samples: {num_total}\")\n",
    "print(f\"Samples with text: {num_non_empty}\")\n",
    "print(f\"Samples without text: {num_empty}\")\n",
    "print(f\"Text coverage: {num_non_empty / num_total:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0ca69",
   "metadata": {},
   "source": [
    "# image\n",
    "* densenet121->emb,rqkmeans->codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb672b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'>\n",
      "dtype: uint8\n",
      "shape: (224, 224)\n",
      "ndim: 2\n",
      "size: 50176\n",
      "first elements: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "img_path = \"/Users/lixiaokang/daily/data/train_samples/images/s_000000.npy\"\n",
    "\n",
    "img = np.load(img_path, allow_pickle=True)\n",
    "\n",
    "print(\"type:\", type(img))\n",
    "print(\"dtype:\", img.dtype)\n",
    "\n",
    "if isinstance(img, np.ndarray):\n",
    "    print(\"shape:\", img.shape)\n",
    "    print(\"ndim:\", img.ndim)\n",
    "    print(\"size:\", img.size)\n",
    "\n",
    "    print(\"first elements:\", img.flatten()[:10])\n",
    "else:\n",
    "    print(\"content:\", img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31c645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.uint8(0), np.uint8(255), np.float64(94.75358737244898))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.min(), img.max(), img.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6eb387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sample_id', 'subject_id', 'meta', 'image', 'tabular', 'timeseries', 'label_any', 'label_multi'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = demo_samples[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c41eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(224, 224)\n",
      "uint8\n",
      "0 255 118.86437739158163\n"
     ]
    }
   ],
   "source": [
    "img = sample[\"image\"]\n",
    "\n",
    "print(type(img))\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "\n",
    "print(img.min(), img.max(), img.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5669312",
   "metadata": {},
   "source": [
    "## densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f33d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class DenseNet121_Encoder(nn.Module):\n",
    "    def __init__(self, out_dim=256, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # DenseNet-121 backbone\n",
    "        self.backbone = models.densenet121(pretrained=pretrained)\n",
    "\n",
    "        self.backbone.features.conv0 = nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "        feat_dim = self.backbone.classifier.in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        self.proj = nn.Linear(feat_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, 224, 224)\n",
    "        return:\n",
    "            z: (B, 256)\n",
    "        \"\"\"\n",
    "        feat = self.backbone(x)        # (B, 1024)\n",
    "        z = self.proj(feat)             # (B, 256)\n",
    "        z = F.normalize(z, dim=-1)     \n",
    "        return z\n",
    "\n",
    "class ImageSemanticModel(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_labels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = DenseNet121_Encoder(out_dim=emb_dim)\n",
    "\n",
    "        self.cls_any = nn.Linear(emb_dim, 1)\n",
    "\n",
    "        self.cls_multi = nn.Linear(emb_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        out_any = self.cls_any(z).squeeze(-1)       # (B,)\n",
    "        out_multi = self.cls_multi(z)               # (B, num_labels)\n",
    "\n",
    "        return {\n",
    "            \"embedding\": z,\n",
    "            \"logit_any\": out_any,\n",
    "            \"logit_multi\": out_multi,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d9a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, demo_samples):\n",
    "        self.samples = demo_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "\n",
    "        img = s[\"image\"]                      # (224,224) uint8\n",
    "        img = img.astype(\"float32\") / 255.0  # normalize\n",
    "        img = torch.from_numpy(img).unsqueeze(0)  # (1,224,224)\n",
    "\n",
    "        label_any = torch.tensor(s[\"label_any\"], dtype=torch.float32)\n",
    "        label_multi = torch.tensor(s[\"label_multi\"], dtype=torch.float32)\n",
    "\n",
    "        return img, label_any, label_multi\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for img, y_any, y_multi in loader:\n",
    "        img = img.to(device)\n",
    "        y_any = y_any.to(device)\n",
    "        y_multi = y_multi.to(device)\n",
    "\n",
    "        out = model(img)\n",
    "\n",
    "        loss_any = criterion_any(out[\"logit_any\"], y_any)\n",
    "        loss_multi = criterion_multi(out[\"logit_multi\"], y_multi)\n",
    "\n",
    "        loss = loss_any + loss_multi\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9defcc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rqenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/rqenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 1.3519\n",
      "Epoch 1: loss = 1.2802\n",
      "Epoch 2: loss = 1.2355\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ImageSemanticModel(emb_dim=256, num_labels=8).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion_any = nn.BCEWithLogitsLoss()\n",
    "criterion_multi = nn.BCEWithLogitsLoss()\n",
    "\n",
    "dataset = ImageDataset(demo_samples)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(loader)\n",
    "    print(f\"Epoch {epoch}: loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f70a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img, _, _ = dataset[0]\n",
    "    z = model.encoder(img.unsqueeze(0).to(device))\n",
    "    print(z.shape)   # (1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9853d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "Z = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, _, _ in loader:\n",
    "        img = img.to(device)\n",
    "        z = model.encoder(img)   # (B, 256)\n",
    "        Z.append(z.cpu())\n",
    "\n",
    "Z = torch.cat(Z, dim=0)   # (N, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b364b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rqvae import RQVAE\n",
    "class ZDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Z.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Z[idx]\n",
    "\n",
    "rq_model = RQVAE(\n",
    "    in_dim=256,                     \n",
    "    num_emb_list=[8, 8, 8],    \n",
    "    e_dim=256,                      \n",
    "    layers=[],                      \n",
    "    dropout_prob=0.0,\n",
    "    bn=False,\n",
    "    loss_type=\"mse\",\n",
    "    quant_loss_weight=1.0,\n",
    "    beta=0.25,\n",
    "    kmeans_init=True,              \n",
    "    kmeans_iters=50,\n",
    "    sk_epsilons=[0.0, 0.0, 0.0],     \n",
    "    sk_iters=50,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(rq_model.parameters(), lr=1e-3)\n",
    "\n",
    "rq_dataset = ZDataset(Z)\n",
    "rq_loader = torch.utils.data.DataLoader(\n",
    "    rq_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20498319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "torch.Size([10, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rqenv/lib/python3.10/site-packages/sklearn/base.py:1365: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "out, rq_loss, indices = rq_model(Z, use_sk=False)\n",
    "\n",
    "print(out.shape)       # (N, 256)\n",
    "print(indices.shape)   # (N, 3)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747c3be",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44130d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_KEEP_COLS = {\n",
    "    \"demographics\": [\"subject_id\", \"gender\", \"anchor_age\", \"anchor_year_group\"],\n",
    "\n",
    "    \"admissions\": [\n",
    "        \"subject_id\", \"hadm_id\", \"admittime\",\n",
    "        \"admission_type\", \"admission_location\",\n",
    "        \"insurance\", \"language\", \"marital_status\",\n",
    "        # \"ethnicity\", \"edregtime\", \"edouttime\"  \n",
    "    ],\n",
    "\n",
    "    \"icustays\": [\"subject_id\", \"hadm_id\", \"stay_id\", \"first_careunit\", \"intime\"],\n",
    "\n",
    "    \"transfers\": [\"subject_id\", \"hadm_id\", \"eventtype\", \"careunit\", \"intime\", \"outtime\"],\n",
    "\n",
    "    \"prescriptions\": [\"subject_id\", \"hadm_id\", \"starttime\", \"drug_type\", \"route\", \"drug\"],\n",
    "\n",
    "    \"procedures_icd\": [\"subject_id\", \"hadm_id\", \"chartdate\", \"icd_code\", \"icd_version\"],\n",
    "\n",
    "    # leak error\n",
    "    # \"diagnoses_icd\": [\"subject_id\", \"hadm_id\", \"icd_code\", \"icd_version\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544dcc0",
   "metadata": {},
   "source": [
    "# tabluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1869d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Table: admissions\n",
      "has_subject_id: True\n",
      "time_cols: ['admittime', 'dischtime', 'deathtime', 'discharge_location', 'edregtime', 'edouttime']\n",
      "num_cols: 16\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'hospital_expire_flag', 'sample_id']\n",
      "================================================================================\n",
      "Table: demographics\n",
      "has_subject_id: True\n",
      "time_cols: ['gender']\n",
      "num_cols: 7\n",
      "columns:\n",
      "['subject_id', 'gender', 'anchor_age', 'anchor_year', 'anchor_year_group', 'dod', 'sample_id']\n",
      "================================================================================\n",
      "Table: diagnoses_icd\n",
      "has_subject_id: True\n",
      "time_cols: []\n",
      "num_cols: 7\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version', 'long_title', 'sample_id']\n",
      "================================================================================\n",
      "Table: icustays\n",
      "has_subject_id: True\n",
      "time_cols: ['intime', 'outtime']\n",
      "num_cols: 9\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'sample_id']\n",
      "================================================================================\n",
      "Table: prescriptions\n",
      "has_subject_id: True\n",
      "time_cols: ['starttime', 'stoptime']\n",
      "num_cols: 18\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'pharmacy_id', 'starttime', 'stoptime', 'drug_type', 'drug', 'gsn', 'ndc', 'prod_strength', 'form_rx', 'dose_val_rx', 'dose_unit_rx', 'form_val_disp', 'form_unit_disp', 'doses_per_24_hrs', 'route', 'sample_id']\n",
      "================================================================================\n",
      "Table: procedures_icd\n",
      "has_subject_id: True\n",
      "time_cols: ['chartdate']\n",
      "num_cols: 8\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'seq_num', 'chartdate', 'icd_code', 'icd_version', 'long_title', 'sample_id']\n",
      "================================================================================\n",
      "Table: transfers\n",
      "has_subject_id: True\n",
      "time_cols: ['intime', 'outtime']\n",
      "num_cols: 8\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'transfer_id', 'eventtype', 'careunit', 'intime', 'outtime', 'sample_id']\n",
      "\n",
      "Saved schema files:\n",
      " - E:/NUS/data/perdata/train_text_samples\\tabular\\_tabular_schema_columns.json\n",
      " - E:/NUS/data/perdata/train_text_samples\\tabular\\_tabular_schema_columns.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \"E:/NUS/data/perdata/train_text_samples\"\n",
    "TAB_DIR = os.path.join(ROOT, \"tabular\")\n",
    "\n",
    "# 输出文件（可选）\n",
    "OUT_JSON = os.path.join(TAB_DIR, \"_tabular_schema_columns.json\")\n",
    "OUT_TXT  = os.path.join(TAB_DIR, \"_tabular_schema_columns.txt\")\n",
    "\n",
    "def scan_tabular_schema(tab_dir: str):\n",
    "    csv_files = sorted([f for f in os.listdir(tab_dir) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "    schema = {}\n",
    "    for fname in csv_files:\n",
    "        path = os.path.join(tab_dir, fname)\n",
    "\n",
    "        info = {\n",
    "            \"file\": fname,\n",
    "            \"path\": path,\n",
    "            \"nrows\": None,          # 行数（需要读全表才能精确，这里用可选方式）\n",
    "            \"has_subject_id\": False,\n",
    "            \"time_cols\": [],\n",
    "            \"columns\": [],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # 只读表头，不读数据\n",
    "            header = pd.read_csv(path, nrows=0)\n",
    "            cols = list(header.columns)\n",
    "            info[\"columns\"] = cols\n",
    "            info[\"has_subject_id\"] = (\"subject_id\" in header.columns)\n",
    "\n",
    "            # 粗略识别可能的时间列名（你也可以扩展关键词）\n",
    "            time_keywords = (\"time\", \"date\", \"admit\", \"discharge\", \"start\", \"end\")\n",
    "            info[\"time_cols\"] = [c for c in cols if any(k in c.lower() for k in time_keywords)]\n",
    "\n",
    "        except Exception as e:\n",
    "            info[\"error\"] = str(e)\n",
    "\n",
    "        schema[fname[:-4]] = info  # key 用表名（去掉 .csv）\n",
    "\n",
    "    return schema\n",
    "\n",
    "schema = scan_tabular_schema(TAB_DIR)\n",
    "\n",
    "# 打印到控制台（方便你快速看）\n",
    "for table, info in schema.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Table: {table}\")\n",
    "    if \"error\" in info:\n",
    "        print(f\"[ERROR] {info['error']}\")\n",
    "        continue\n",
    "    print(f\"has_subject_id: {info['has_subject_id']}\")\n",
    "    print(f\"time_cols: {info['time_cols']}\")\n",
    "    print(f\"num_cols: {len(info['columns'])}\")\n",
    "    print(\"columns:\")\n",
    "    print(info[\"columns\"])\n",
    "\n",
    "# 保存成 json（方便你后面人工筛选/写配置）\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 同时保存成 txt（更适合肉眼扫）\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for table, info in schema.items():\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Table: {table}\\n\")\n",
    "        if \"error\" in info:\n",
    "            f.write(f\"[ERROR] {info['error']}\\n\")\n",
    "            continue\n",
    "        f.write(f\"has_subject_id: {info['has_subject_id']}\\n\")\n",
    "        f.write(f\"time_cols: {info['time_cols']}\\n\")\n",
    "        f.write(f\"num_cols: {len(info['columns'])}\\n\")\n",
    "        f.write(\"columns:\\n\")\n",
    "        f.write(\", \".join(info[\"columns\"]) + \"\\n\\n\")\n",
    "\n",
    "print(\"\\nSaved schema files:\")\n",
    "print(\" -\", OUT_JSON)\n",
    "print(\" -\", OUT_TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75db1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fbee5a",
   "metadata": {},
   "source": [
    "# timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954ac531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Table: chartevents\n",
      "has_subject_id: True\n",
      "time_cols_exact: ['charttime', 'storetime']\n",
      "time_cols_keyword: ['charttime', 'storetime', 'deltacharttime', 'cxr_time']\n",
      "num_cols: 22\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'stay_id', 'charttime', 'storetime', 'itemid', 'value', 'valuenum', 'valueuom', 'warning', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue', 'deltacharttime', 'sample_id', 'cxr_time', 'delta_hours']\n",
      "================================================================================\n",
      "Table: inputevents\n",
      "has_subject_id: True\n",
      "time_cols_exact: ['starttime', 'endtime', 'storetime']\n",
      "time_cols_keyword: ['starttime', 'endtime', 'storetime', 'cxr_time']\n",
      "num_cols: 37\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'stay_id', 'starttime', 'endtime', 'storetime', 'itemid', 'amount', 'amountuom', 'rate', 'rateuom', 'orderid', 'linkorderid', 'ordercategoryname', 'secondaryordercategoryname', 'ordercomponenttypedescription', 'ordercategorydescription', 'patientweight', 'totalamount', 'totalamountuom', 'isopenbag', 'continueinnextdept', 'cancelreason', 'statusdescription', 'originalamount', 'originalrate', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue', 'sample_id', 'cxr_time', 'delta_hours']\n",
      "================================================================================\n",
      "Table: labevents\n",
      "has_subject_id: True\n",
      "time_cols_exact: ['charttime', 'storetime']\n",
      "time_cols_keyword: ['charttime', 'storetime', 'deltacharttime', 'cxr_time']\n",
      "num_cols: 23\n",
      "columns:\n",
      "['labevent_id', 'subject_id', 'hadm_id', 'specimen_id', 'itemid', 'charttime', 'storetime', 'value', 'valuenum', 'valueuom', 'ref_range_lower', 'ref_range_upper', 'flag', 'priority', 'comments', 'label', 'fluid', 'category', 'loinc_code', 'deltacharttime', 'sample_id', 'cxr_time', 'delta_hours']\n",
      "================================================================================\n",
      "Table: outputevents\n",
      "has_subject_id: True\n",
      "time_cols_exact: ['charttime', 'storetime']\n",
      "time_cols_keyword: ['charttime', 'storetime', 'deltacharttime', 'cxr_time']\n",
      "num_cols: 20\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'stay_id', 'charttime', 'storetime', 'itemid', 'value', 'valueuom', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue', 'deltacharttime', 'sample_id', 'cxr_time', 'delta_hours']\n",
      "================================================================================\n",
      "Table: procedureevents\n",
      "has_subject_id: True\n",
      "time_cols_exact: ['starttime', 'endtime', 'storetime']\n",
      "time_cols_keyword: ['starttime', 'endtime', 'storetime', 'comments_date', 'cxr_time']\n",
      "num_cols: 37\n",
      "columns:\n",
      "['subject_id', 'hadm_id', 'stay_id', 'starttime', 'endtime', 'storetime', 'itemid', 'value', 'valueuom', 'location', 'locationcategory', 'orderid', 'linkorderid', 'ordercategoryname', 'secondaryordercategoryname', 'ordercategorydescription', 'patientweight', 'totalamount', 'totalamountuom', 'isopenbag', 'continueinnextdept', 'cancelreason', 'statusdescription', 'comments_date', 'originalamount', 'originalrate', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue', 'sample_id', 'cxr_time', 'delta_hours']\n",
      "\n",
      "Saved schema files:\n",
      " - E:/NUS/data/perdata/train_text_samples\\timeseries_merged\\_timeseries_schema_columns.json\n",
      " - E:/NUS/data/perdata/train_text_samples\\timeseries_merged\\_timeseries_schema_columns.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \"E:/NUS/data/perdata/train_text_samples\"\n",
    "TS_DIR = os.path.join(ROOT, \"timeseries_merged\")\n",
    "\n",
    "# 输出文件（可选）\n",
    "OUT_JSON = os.path.join(TS_DIR, \"_timeseries_schema_columns.json\")\n",
    "OUT_TXT  = os.path.join(TS_DIR, \"_timeseries_schema_columns.txt\")\n",
    "\n",
    "\n",
    "def scan_timeseries_schema(ts_dir: str):\n",
    "    csv_files = sorted([f for f in os.listdir(ts_dir) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "    schema = {}\n",
    "    for fname in csv_files:\n",
    "        path = os.path.join(ts_dir, fname)\n",
    "\n",
    "        info = {\n",
    "            \"file\": fname,\n",
    "            \"path\": path,\n",
    "            \"has_subject_id\": False,\n",
    "            \"time_cols_exact\": [],   # 常见时间列名命中\n",
    "            \"time_cols_keyword\": [], # 关键词匹配出来的\n",
    "            \"columns\": [],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # 只读表头\n",
    "            header = pd.read_csv(path, nrows=0)\n",
    "            cols = list(header.columns)\n",
    "            info[\"columns\"] = cols\n",
    "            info[\"has_subject_id\"] = (\"subject_id\" in header.columns)\n",
    "\n",
    "            # 1) 常见时间列名（timeseries 通常就这些）\n",
    "            exact_time_names = {\n",
    "                \"charttime\", \"storetime\",\n",
    "                \"starttime\", \"stoptime\",\n",
    "                \"intime\", \"outtime\",\n",
    "                \"eventtime\", \"endtime\",\n",
    "                \"chartdate\", \"eventdate\",\n",
    "                \"admittime\", \"dischtime\",\n",
    "            }\n",
    "            info[\"time_cols_exact\"] = [c for c in cols if c.lower() in exact_time_names]\n",
    "\n",
    "            # 2) 关键词匹配（兜底）\n",
    "            time_keywords = (\"time\", \"date\")\n",
    "            info[\"time_cols_keyword\"] = [\n",
    "                c for c in cols\n",
    "                if any(k in c.lower() for k in time_keywords)\n",
    "            ]\n",
    "\n",
    "        except Exception as e:\n",
    "            info[\"error\"] = str(e)\n",
    "\n",
    "        schema[fname[:-4]] = info  # key 用表名（去掉 .csv）\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "schema = scan_timeseries_schema(TS_DIR)\n",
    "\n",
    "# 打印到控制台\n",
    "for table, info in schema.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Table: {table}\")\n",
    "    if \"error\" in info:\n",
    "        print(f\"[ERROR] {info['error']}\")\n",
    "        continue\n",
    "    print(f\"has_subject_id: {info['has_subject_id']}\")\n",
    "    print(f\"time_cols_exact: {info['time_cols_exact']}\")\n",
    "    print(f\"time_cols_keyword: {info['time_cols_keyword']}\")\n",
    "    print(f\"num_cols: {len(info['columns'])}\")\n",
    "    print(\"columns:\")\n",
    "    print(info[\"columns\"])\n",
    "\n",
    "# 保存成 json\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 保存成 txt\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for table, info in schema.items():\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Table: {table}\\n\")\n",
    "        if \"error\" in info:\n",
    "            f.write(f\"[ERROR] {info['error']}\\n\")\n",
    "            continue\n",
    "        f.write(f\"has_subject_id: {info['has_subject_id']}\\n\")\n",
    "        f.write(f\"time_cols_exact: {info['time_cols_exact']}\\n\")\n",
    "        f.write(f\"time_cols_keyword: {info['time_cols_keyword']}\\n\")\n",
    "        f.write(f\"num_cols: {len(info['columns'])}\\n\")\n",
    "        f.write(\"columns:\\n\")\n",
    "        f.write(\", \".join(info[\"columns\"]) + \"\\n\\n\")\n",
    "\n",
    "print(\"\\nSaved schema files:\")\n",
    "print(\" -\", OUT_JSON)\n",
    "print(\" -\", OUT_TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "971515b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               file      size     rows  cols                                                                             path\n",
      "    chartevents.csv   6.53 GB 30153901    22     E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\\chartevents.csv\n",
      "    inputevents.csv   6.49 GB 17881065    37     E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\\inputevents.csv\n",
      "      labevents.csv 601.64 MB  2916563    23       E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\\labevents.csv\n",
      "procedureevents.csv 371.39 MB  1151392    37 E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\\procedureevents.csv\n",
      "   outputevents.csv  78.75 MB   427810    20    E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\\outputevents.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = r\"E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\"\n",
    "\n",
    "def count_rows_fast(csv_path: str):\n",
    "    # 逐行计数，几乎不占内存；一般 MIMIC 这种 CSV 没“字段内换行”，可放心用\n",
    "    n = 0\n",
    "    with open(csv_path, \"rb\") as f:\n",
    "        for _ in f:\n",
    "            n += 1\n",
    "    return max(0, n - 1)  # 减 header\n",
    "\n",
    "def human_size(num_bytes: int):\n",
    "    gb = num_bytes / (1024**3)\n",
    "    mb = num_bytes / (1024**2)\n",
    "    return (f\"{gb:.2f} GB\" if gb >= 1 else f\"{mb:.2f} MB\")\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(glob.glob(os.path.join(ROOT, \"*.csv\"))):\n",
    "    size_bytes = os.path.getsize(fp)\n",
    "    # 只读 header 拿列数\n",
    "    cols = pd.read_csv(fp, nrows=0, low_memory=False).shape[1]\n",
    "    nrows = count_rows_fast(fp)\n",
    "\n",
    "    rows.append({\n",
    "        \"file\": os.path.basename(fp),\n",
    "        \"size\": human_size(size_bytes),\n",
    "        \"rows\": nrows,\n",
    "        \"cols\": cols,\n",
    "        \"path\": fp,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"rows\", \"file\"], ascending=[False, True])\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "print(df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd2989d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TS-BUCKET] table=chartevents file=chartevents.csv\n",
      "  chunk 0: rows=100,000\n",
      "  chunk 1: rows=100,000\n",
      "  chunk 2: rows=100,000\n",
      "  chunk 3: rows=100,000\n",
      "  chunk 4: rows=100,000\n",
      "  chunk 5: rows=100,000\n",
      "  chunk 6: rows=100,000\n",
      "  chunk 7: rows=100,000\n",
      "  chunk 8: rows=100,000\n",
      "  chunk 9: rows=100,000\n",
      "  chunk 10: rows=100,000\n",
      "  chunk 11: rows=100,000\n",
      "  chunk 12: rows=100,000\n",
      "  chunk 13: rows=100,000\n",
      "  chunk 14: rows=100,000\n",
      "  chunk 15: rows=100,000\n",
      "  chunk 16: rows=100,000\n",
      "  chunk 17: rows=100,000\n",
      "  chunk 18: rows=100,000\n",
      "  chunk 19: rows=100,000\n",
      "  chunk 20: rows=100,000\n",
      "  chunk 21: rows=100,000\n",
      "  chunk 22: rows=100,000\n",
      "  chunk 23: rows=100,000\n",
      "  chunk 24: rows=100,000\n",
      "  chunk 25: rows=100,000\n",
      "  chunk 26: rows=100,000\n",
      "  chunk 27: rows=100,000\n",
      "  chunk 28: rows=100,000\n",
      "  chunk 29: rows=100,000\n",
      "  chunk 30: rows=100,000\n",
      "  chunk 31: rows=100,000\n",
      "  chunk 32: rows=100,000\n",
      "  chunk 33: rows=100,000\n",
      "  chunk 34: rows=100,000\n",
      "  chunk 35: rows=100,000\n",
      "  chunk 36: rows=100,000\n",
      "  chunk 37: rows=100,000\n",
      "  chunk 38: rows=100,000\n",
      "  chunk 39: rows=100,000\n",
      "  chunk 40: rows=100,000\n",
      "  chunk 41: rows=100,000\n",
      "  chunk 42: rows=100,000\n",
      "  chunk 43: rows=100,000\n",
      "  chunk 44: rows=100,000\n",
      "  chunk 45: rows=100,000\n",
      "  chunk 46: rows=100,000\n",
      "  chunk 47: rows=100,000\n",
      "  chunk 48: rows=100,000\n",
      "  chunk 49: rows=100,000\n",
      "  chunk 50: rows=100,000\n",
      "  chunk 51: rows=100,000\n",
      "  chunk 52: rows=100,000\n",
      "  chunk 53: rows=100,000\n",
      "  chunk 54: rows=100,000\n",
      "  chunk 55: rows=100,000\n",
      "  chunk 56: rows=100,000\n",
      "  chunk 57: rows=100,000\n",
      "  chunk 58: rows=100,000\n",
      "  chunk 59: rows=100,000\n",
      "  chunk 60: rows=100,000\n",
      "  chunk 61: rows=100,000\n",
      "  chunk 62: rows=100,000\n",
      "  chunk 63: rows=100,000\n",
      "  chunk 64: rows=100,000\n",
      "  chunk 65: rows=100,000\n",
      "  chunk 66: rows=100,000\n",
      "  chunk 67: rows=100,000\n",
      "  chunk 68: rows=100,000\n",
      "  chunk 69: rows=100,000\n",
      "  chunk 70: rows=100,000\n",
      "  chunk 71: rows=100,000\n",
      "  chunk 72: rows=100,000\n",
      "  chunk 73: rows=100,000\n",
      "  chunk 74: rows=100,000\n",
      "  chunk 75: rows=100,000\n",
      "  chunk 76: rows=100,000\n",
      "  chunk 77: rows=100,000\n",
      "  chunk 78: rows=100,000\n",
      "  chunk 79: rows=100,000\n",
      "  chunk 80: rows=100,000\n",
      "  chunk 81: rows=100,000\n",
      "  chunk 82: rows=100,000\n",
      "  chunk 83: rows=100,000\n",
      "  chunk 84: rows=100,000\n",
      "  chunk 85: rows=100,000\n",
      "  chunk 86: rows=100,000\n",
      "  chunk 87: rows=100,000\n",
      "  chunk 88: rows=100,000\n",
      "  chunk 89: rows=100,000\n",
      "  chunk 90: rows=100,000\n",
      "  chunk 91: rows=100,000\n",
      "  chunk 92: rows=100,000\n",
      "  chunk 93: rows=100,000\n",
      "  chunk 94: rows=100,000\n",
      "  chunk 95: rows=100,000\n",
      "  chunk 96: rows=100,000\n",
      "  chunk 97: rows=100,000\n",
      "  chunk 98: rows=100,000\n",
      "  chunk 99: rows=100,000\n",
      "  chunk 100: rows=100,000\n",
      "  chunk 101: rows=100,000\n",
      "  chunk 102: rows=100,000\n",
      "  chunk 103: rows=100,000\n",
      "  chunk 104: rows=100,000\n",
      "  chunk 105: rows=100,000\n",
      "  chunk 106: rows=100,000\n",
      "  chunk 107: rows=100,000\n",
      "  chunk 108: rows=100,000\n",
      "  chunk 109: rows=100,000\n",
      "  chunk 110: rows=100,000\n",
      "  chunk 111: rows=100,000\n",
      "  chunk 112: rows=100,000\n",
      "  chunk 113: rows=100,000\n",
      "  chunk 114: rows=100,000\n",
      "  chunk 115: rows=100,000\n",
      "  chunk 116: rows=100,000\n",
      "  chunk 117: rows=100,000\n",
      "  chunk 118: rows=100,000\n",
      "  chunk 119: rows=100,000\n",
      "  chunk 120: rows=100,000\n",
      "  chunk 121: rows=100,000\n",
      "  chunk 122: rows=100,000\n",
      "  chunk 123: rows=100,000\n",
      "  chunk 124: rows=100,000\n",
      "  chunk 125: rows=100,000\n",
      "  chunk 126: rows=100,000\n",
      "  chunk 127: rows=100,000\n",
      "  chunk 128: rows=100,000\n",
      "  chunk 129: rows=100,000\n",
      "  chunk 130: rows=100,000\n",
      "  chunk 131: rows=100,000\n",
      "  chunk 132: rows=100,000\n",
      "  chunk 133: rows=100,000\n",
      "  chunk 134: rows=100,000\n",
      "  chunk 135: rows=100,000\n",
      "  chunk 136: rows=100,000\n",
      "  chunk 137: rows=100,000\n",
      "  chunk 138: rows=100,000\n",
      "  chunk 139: rows=100,000\n",
      "  chunk 140: rows=100,000\n",
      "  chunk 141: rows=100,000\n",
      "  chunk 142: rows=100,000\n",
      "  chunk 143: rows=100,000\n",
      "  chunk 144: rows=100,000\n",
      "  chunk 145: rows=100,000\n",
      "  chunk 146: rows=100,000\n",
      "  chunk 147: rows=100,000\n",
      "  chunk 148: rows=100,000\n",
      "  chunk 149: rows=100,000\n",
      "  chunk 150: rows=100,000\n",
      "  chunk 151: rows=100,000\n",
      "  chunk 152: rows=100,000\n",
      "  chunk 153: rows=100,000\n",
      "  chunk 154: rows=100,000\n",
      "  chunk 155: rows=100,000\n",
      "  chunk 156: rows=100,000\n",
      "  chunk 157: rows=100,000\n",
      "  chunk 158: rows=100,000\n",
      "  chunk 159: rows=100,000\n",
      "  chunk 160: rows=100,000\n",
      "  chunk 161: rows=100,000\n",
      "  chunk 162: rows=100,000\n",
      "  chunk 163: rows=100,000\n",
      "  chunk 164: rows=100,000\n",
      "  chunk 165: rows=100,000\n",
      "  chunk 166: rows=100,000\n",
      "  chunk 167: rows=100,000\n",
      "  chunk 168: rows=100,000\n",
      "  chunk 169: rows=100,000\n",
      "  chunk 170: rows=100,000\n",
      "  chunk 171: rows=100,000\n",
      "  chunk 172: rows=100,000\n",
      "  chunk 173: rows=100,000\n",
      "  chunk 174: rows=100,000\n",
      "  chunk 175: rows=100,000\n",
      "  chunk 176: rows=100,000\n",
      "  chunk 177: rows=100,000\n",
      "  chunk 178: rows=100,000\n",
      "  chunk 179: rows=100,000\n",
      "  chunk 180: rows=100,000\n",
      "  chunk 181: rows=100,000\n",
      "  chunk 182: rows=100,000\n",
      "  chunk 183: rows=100,000\n",
      "  chunk 184: rows=100,000\n",
      "  chunk 185: rows=100,000\n",
      "  chunk 186: rows=100,000\n",
      "  chunk 187: rows=100,000\n",
      "  chunk 188: rows=100,000\n",
      "  chunk 189: rows=100,000\n",
      "  chunk 190: rows=100,000\n",
      "  chunk 191: rows=100,000\n",
      "  chunk 192: rows=100,000\n",
      "  chunk 193: rows=100,000\n",
      "  chunk 194: rows=100,000\n",
      "  chunk 195: rows=100,000\n",
      "  chunk 196: rows=100,000\n",
      "  chunk 197: rows=100,000\n",
      "  chunk 198: rows=100,000\n",
      "  chunk 199: rows=100,000\n",
      "  chunk 200: rows=100,000\n",
      "  chunk 201: rows=100,000\n",
      "  chunk 202: rows=100,000\n",
      "  chunk 203: rows=100,000\n",
      "  chunk 204: rows=100,000\n",
      "  chunk 205: rows=100,000\n",
      "  chunk 206: rows=100,000\n",
      "  chunk 207: rows=100,000\n",
      "  chunk 208: rows=100,000\n",
      "  chunk 209: rows=100,000\n",
      "  chunk 210: rows=100,000\n",
      "  chunk 211: rows=100,000\n",
      "  chunk 212: rows=100,000\n",
      "  chunk 213: rows=100,000\n",
      "  chunk 214: rows=100,000\n",
      "  chunk 215: rows=100,000\n",
      "  chunk 216: rows=100,000\n",
      "  chunk 217: rows=100,000\n",
      "  chunk 218: rows=100,000\n",
      "  chunk 219: rows=100,000\n",
      "  chunk 220: rows=100,000\n",
      "  chunk 221: rows=100,000\n",
      "  chunk 222: rows=100,000\n",
      "  chunk 223: rows=100,000\n",
      "  chunk 224: rows=100,000\n",
      "  chunk 225: rows=100,000\n",
      "  chunk 226: rows=100,000\n",
      "  chunk 227: rows=100,000\n",
      "  chunk 228: rows=100,000\n",
      "  chunk 229: rows=100,000\n",
      "  chunk 230: rows=100,000\n",
      "  chunk 231: rows=100,000\n",
      "  chunk 232: rows=100,000\n",
      "  chunk 233: rows=100,000\n",
      "  chunk 234: rows=100,000\n",
      "  chunk 235: rows=100,000\n",
      "  chunk 236: rows=100,000\n",
      "  chunk 237: rows=100,000\n",
      "  chunk 238: rows=100,000\n",
      "  chunk 239: rows=100,000\n",
      "  chunk 240: rows=100,000\n",
      "  chunk 241: rows=100,000\n",
      "  chunk 242: rows=100,000\n",
      "  chunk 243: rows=100,000\n",
      "  chunk 244: rows=100,000\n",
      "  chunk 245: rows=100,000\n",
      "  chunk 246: rows=100,000\n",
      "  chunk 247: rows=100,000\n",
      "  chunk 248: rows=100,000\n",
      "  chunk 249: rows=100,000\n",
      "  chunk 250: rows=100,000\n",
      "  chunk 251: rows=100,000\n",
      "  chunk 252: rows=100,000\n",
      "  chunk 253: rows=100,000\n",
      "  chunk 254: rows=100,000\n",
      "  chunk 255: rows=100,000\n",
      "  chunk 256: rows=100,000\n",
      "  chunk 257: rows=100,000\n",
      "  chunk 258: rows=100,000\n",
      "  chunk 259: rows=100,000\n",
      "  chunk 260: rows=100,000\n",
      "  chunk 261: rows=100,000\n",
      "  chunk 262: rows=100,000\n",
      "  chunk 263: rows=100,000\n",
      "  chunk 264: rows=100,000\n",
      "  chunk 265: rows=100,000\n",
      "  chunk 266: rows=100,000\n",
      "  chunk 267: rows=100,000\n",
      "  chunk 268: rows=100,000\n",
      "  chunk 269: rows=100,000\n",
      "  chunk 270: rows=100,000\n",
      "  chunk 271: rows=100,000\n",
      "  chunk 272: rows=100,000\n",
      "  chunk 273: rows=100,000\n",
      "  chunk 274: rows=100,000\n",
      "  chunk 275: rows=100,000\n",
      "  chunk 276: rows=100,000\n",
      "  chunk 277: rows=100,000\n",
      "  chunk 278: rows=100,000\n",
      "  chunk 279: rows=100,000\n",
      "  chunk 280: rows=100,000\n",
      "  chunk 281: rows=100,000\n",
      "  chunk 282: rows=100,000\n",
      "  chunk 283: rows=100,000\n",
      "  chunk 284: rows=100,000\n",
      "  chunk 285: rows=100,000\n",
      "  chunk 286: rows=100,000\n",
      "  chunk 287: rows=100,000\n",
      "  chunk 288: rows=100,000\n",
      "  chunk 289: rows=100,000\n",
      "  chunk 290: rows=100,000\n",
      "  chunk 291: rows=100,000\n",
      "  chunk 292: rows=100,000\n",
      "  chunk 293: rows=100,000\n",
      "  chunk 294: rows=100,000\n",
      "  chunk 295: rows=100,000\n",
      "  chunk 296: rows=100,000\n",
      "  chunk 297: rows=100,000\n",
      "  chunk 298: rows=100,000\n",
      "  chunk 299: rows=100,000\n",
      "  chunk 300: rows=100,000\n",
      "  chunk 301: rows=53,901\n",
      "[TS-BUCKET] done table=chartevents\n",
      "\n",
      "[TS-BUCKET] table=inputevents file=inputevents.csv\n",
      "  chunk 0: rows=100,000\n",
      "  chunk 1: rows=100,000\n",
      "  chunk 2: rows=100,000\n",
      "  chunk 3: rows=100,000\n",
      "  chunk 4: rows=100,000\n",
      "  chunk 5: rows=100,000\n",
      "  chunk 6: rows=100,000\n",
      "  chunk 7: rows=100,000\n",
      "  chunk 8: rows=100,000\n",
      "  chunk 9: rows=100,000\n",
      "  chunk 10: rows=100,000\n",
      "  chunk 11: rows=100,000\n",
      "  chunk 12: rows=100,000\n",
      "  chunk 13: rows=100,000\n",
      "  chunk 14: rows=100,000\n",
      "  chunk 15: rows=100,000\n",
      "  chunk 16: rows=100,000\n",
      "  chunk 17: rows=100,000\n",
      "  chunk 18: rows=100,000\n",
      "  chunk 19: rows=100,000\n",
      "  chunk 20: rows=100,000\n",
      "  chunk 21: rows=100,000\n",
      "  chunk 22: rows=100,000\n",
      "  chunk 23: rows=100,000\n",
      "  chunk 24: rows=100,000\n",
      "  chunk 25: rows=100,000\n",
      "  chunk 26: rows=100,000\n",
      "  chunk 27: rows=100,000\n",
      "  chunk 28: rows=100,000\n",
      "  chunk 29: rows=100,000\n",
      "  chunk 30: rows=100,000\n",
      "  chunk 31: rows=100,000\n",
      "  chunk 32: rows=100,000\n",
      "  chunk 33: rows=100,000\n",
      "  chunk 34: rows=100,000\n",
      "  chunk 35: rows=100,000\n",
      "  chunk 36: rows=100,000\n",
      "  chunk 37: rows=100,000\n",
      "  chunk 38: rows=100,000\n",
      "  chunk 39: rows=100,000\n",
      "  chunk 40: rows=100,000\n",
      "  chunk 41: rows=100,000\n",
      "  chunk 42: rows=100,000\n",
      "  chunk 43: rows=100,000\n",
      "  chunk 44: rows=100,000\n",
      "  chunk 45: rows=100,000\n",
      "  chunk 46: rows=100,000\n",
      "  chunk 47: rows=100,000\n",
      "  chunk 48: rows=100,000\n",
      "  chunk 49: rows=100,000\n",
      "  chunk 50: rows=100,000\n",
      "  chunk 51: rows=100,000\n",
      "  chunk 52: rows=100,000\n",
      "  chunk 53: rows=100,000\n",
      "  chunk 54: rows=100,000\n",
      "  chunk 55: rows=100,000\n",
      "  chunk 56: rows=100,000\n",
      "  chunk 57: rows=100,000\n",
      "  chunk 58: rows=100,000\n",
      "  chunk 59: rows=100,000\n",
      "  chunk 60: rows=100,000\n",
      "  chunk 61: rows=100,000\n",
      "  chunk 62: rows=100,000\n",
      "  chunk 63: rows=100,000\n",
      "  chunk 64: rows=100,000\n",
      "  chunk 65: rows=100,000\n",
      "  chunk 66: rows=100,000\n",
      "  chunk 67: rows=100,000\n",
      "  chunk 68: rows=100,000\n",
      "  chunk 69: rows=100,000\n",
      "  chunk 70: rows=100,000\n",
      "  chunk 71: rows=100,000\n",
      "  chunk 72: rows=100,000\n",
      "  chunk 73: rows=100,000\n",
      "  chunk 74: rows=100,000\n",
      "  chunk 75: rows=100,000\n",
      "  chunk 76: rows=100,000\n",
      "  chunk 77: rows=100,000\n",
      "  chunk 78: rows=100,000\n",
      "  chunk 79: rows=100,000\n",
      "  chunk 80: rows=100,000\n",
      "  chunk 81: rows=100,000\n",
      "  chunk 82: rows=100,000\n",
      "  chunk 83: rows=100,000\n",
      "  chunk 84: rows=100,000\n",
      "  chunk 85: rows=100,000\n",
      "  chunk 86: rows=100,000\n",
      "  chunk 87: rows=100,000\n",
      "  chunk 88: rows=100,000\n",
      "  chunk 89: rows=100,000\n",
      "  chunk 90: rows=100,000\n",
      "  chunk 91: rows=100,000\n",
      "  chunk 92: rows=100,000\n",
      "  chunk 93: rows=100,000\n",
      "  chunk 94: rows=100,000\n",
      "  chunk 95: rows=100,000\n",
      "  chunk 96: rows=100,000\n",
      "  chunk 97: rows=100,000\n",
      "  chunk 98: rows=100,000\n",
      "  chunk 99: rows=100,000\n",
      "  chunk 100: rows=100,000\n",
      "  chunk 101: rows=100,000\n",
      "  chunk 102: rows=100,000\n",
      "  chunk 103: rows=100,000\n",
      "  chunk 104: rows=100,000\n",
      "  chunk 105: rows=100,000\n",
      "  chunk 106: rows=100,000\n",
      "  chunk 107: rows=100,000\n",
      "  chunk 108: rows=100,000\n",
      "  chunk 109: rows=100,000\n",
      "  chunk 110: rows=100,000\n",
      "  chunk 111: rows=100,000\n",
      "  chunk 112: rows=100,000\n",
      "  chunk 113: rows=100,000\n",
      "  chunk 114: rows=100,000\n",
      "  chunk 115: rows=100,000\n",
      "  chunk 116: rows=100,000\n",
      "  chunk 117: rows=100,000\n",
      "  chunk 118: rows=100,000\n",
      "  chunk 119: rows=100,000\n",
      "  chunk 120: rows=100,000\n",
      "  chunk 121: rows=100,000\n",
      "  chunk 122: rows=100,000\n",
      "  chunk 123: rows=100,000\n",
      "  chunk 124: rows=100,000\n",
      "  chunk 125: rows=100,000\n",
      "  chunk 126: rows=100,000\n",
      "  chunk 127: rows=100,000\n",
      "  chunk 128: rows=100,000\n",
      "  chunk 129: rows=100,000\n",
      "  chunk 130: rows=100,000\n",
      "  chunk 131: rows=100,000\n",
      "  chunk 132: rows=100,000\n",
      "  chunk 133: rows=100,000\n",
      "  chunk 134: rows=100,000\n",
      "  chunk 135: rows=100,000\n",
      "  chunk 136: rows=100,000\n",
      "  chunk 137: rows=100,000\n",
      "  chunk 138: rows=100,000\n",
      "  chunk 139: rows=100,000\n",
      "  chunk 140: rows=100,000\n",
      "  chunk 141: rows=100,000\n",
      "  chunk 142: rows=100,000\n",
      "  chunk 143: rows=100,000\n",
      "  chunk 144: rows=100,000\n",
      "  chunk 145: rows=100,000\n",
      "  chunk 146: rows=100,000\n",
      "  chunk 147: rows=100,000\n",
      "  chunk 148: rows=100,000\n",
      "  chunk 149: rows=100,000\n",
      "  chunk 150: rows=100,000\n",
      "  chunk 151: rows=100,000\n",
      "  chunk 152: rows=100,000\n",
      "  chunk 153: rows=100,000\n",
      "  chunk 154: rows=100,000\n",
      "  chunk 155: rows=100,000\n",
      "  chunk 156: rows=100,000\n",
      "  chunk 157: rows=100,000\n",
      "  chunk 158: rows=100,000\n",
      "  chunk 159: rows=100,000\n",
      "  chunk 160: rows=100,000\n",
      "  chunk 161: rows=100,000\n",
      "  chunk 162: rows=100,000\n",
      "  chunk 163: rows=100,000\n",
      "  chunk 164: rows=100,000\n",
      "  chunk 165: rows=100,000\n",
      "  chunk 166: rows=100,000\n",
      "  chunk 167: rows=100,000\n",
      "  chunk 168: rows=100,000\n",
      "  chunk 169: rows=100,000\n",
      "  chunk 170: rows=100,000\n",
      "  chunk 171: rows=100,000\n",
      "  chunk 172: rows=100,000\n",
      "  chunk 173: rows=100,000\n",
      "  chunk 174: rows=100,000\n",
      "  chunk 175: rows=100,000\n",
      "  chunk 176: rows=100,000\n",
      "  chunk 177: rows=100,000\n",
      "  chunk 178: rows=81,065\n",
      "[TS-BUCKET] done table=inputevents\n",
      "\n",
      "[TS-BUCKET] table=labevents file=labevents.csv\n",
      "  chunk 0: rows=100,000\n",
      "  chunk 1: rows=100,000\n",
      "  chunk 2: rows=100,000\n",
      "  chunk 3: rows=100,000\n",
      "  chunk 4: rows=100,000\n",
      "  chunk 5: rows=100,000\n",
      "  chunk 6: rows=100,000\n",
      "  chunk 7: rows=100,000\n",
      "  chunk 8: rows=100,000\n",
      "  chunk 9: rows=100,000\n",
      "  chunk 10: rows=100,000\n",
      "  chunk 11: rows=100,000\n",
      "  chunk 12: rows=100,000\n",
      "  chunk 13: rows=100,000\n",
      "  chunk 14: rows=100,000\n",
      "  chunk 15: rows=100,000\n",
      "  chunk 16: rows=100,000\n",
      "  chunk 17: rows=100,000\n",
      "  chunk 18: rows=100,000\n",
      "  chunk 19: rows=100,000\n",
      "  chunk 20: rows=100,000\n",
      "  chunk 21: rows=100,000\n",
      "  chunk 22: rows=100,000\n",
      "  chunk 23: rows=100,000\n",
      "  chunk 24: rows=100,000\n",
      "  chunk 25: rows=100,000\n",
      "  chunk 26: rows=100,000\n",
      "  chunk 27: rows=100,000\n",
      "  chunk 28: rows=100,000\n",
      "  chunk 29: rows=16,563\n",
      "[TS-BUCKET] done table=labevents\n",
      "\n",
      "[TS-BUCKET] table=outputevents file=outputevents.csv\n",
      "  chunk 0: rows=100,000\n",
      "  chunk 1: rows=100,000\n",
      "  chunk 2: rows=100,000\n",
      "  chunk 3: rows=100,000\n",
      "  chunk 4: rows=27,810\n",
      "[TS-BUCKET] done table=outputevents\n",
      "\n",
      "[TS-BUCKET] table=procedureevents file=procedureevents.csv\n",
      "  chunk 0: rows=100,000\n",
      "  chunk 1: rows=100,000\n",
      "  chunk 2: rows=100,000\n",
      "  chunk 3: rows=100,000\n",
      "  chunk 4: rows=100,000\n",
      "  chunk 5: rows=100,000\n",
      "  chunk 6: rows=100,000\n",
      "  chunk 7: rows=100,000\n",
      "  chunk 8: rows=100,000\n",
      "  chunk 9: rows=100,000\n",
      "  chunk 10: rows=100,000\n",
      "  chunk 11: rows=51,392\n",
      "[TS-BUCKET] done table=procedureevents\n"
     ]
    }
   ],
   "source": [
    "from dataset import bucketize_timeseries_csv\n",
    "\n",
    "bucketize_timeseries_csv(\n",
    "    TS_DIR=\"E:/NUS/data/perdata/train_text_all_samples/timeseries_merged\",\n",
    "    OUT_DIR=\"E:/NUS/data/perdata/train_text_all_samples/timeseries_bucketed\",\n",
    "    num_buckets=128,\n",
    "    chunksize=100_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27826188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 109968\n",
      "[TAB] demographics: rows=109,968 cols=4\n",
      "[TAB] admissions: rows=109,968 cols=8\n",
      "[TAB] icustays: rows=109,968 cols=6\n",
      "[TAB] transfers: rows=664,791 cols=6\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m df_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sample_index)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_index))\n\u001b[1;32m--> 160\u001b[0m tab_df, tab_idx \u001b[38;5;241m=\u001b[39m \u001b[43mload_tabular_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTAB_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m _, text_map_dicom, text_map_subject \u001b[38;5;241m=\u001b[39m load_text_table(TEXT_PATH)\n\u001b[0;32m    163\u001b[0m ts_reader \u001b[38;5;241m=\u001b[39m TimeseriesBucketReader(\n\u001b[0;32m    164\u001b[0m     bucket_root\u001b[38;5;241m=\u001b[39mTS_BUCKET_ROOT,\n\u001b[0;32m    165\u001b[0m     num_buckets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,   \u001b[38;5;66;03m# 这里要和你 bucketize 一致\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m    167\u001b[0m )\n",
      "Cell \u001b[1;32mIn[47], line 38\u001b[0m, in \u001b[0;36mload_tabular_all\u001b[1;34m(TAB_DIR)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[TAB][SKIP] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv_light\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musecols\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparse_dates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     47\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 16\u001b[0m, in \u001b[0;36mread_csv_light\u001b[1;34m(path, usecols, dtype, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_csv_light\u001b[39m(path, usecols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\grmenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\grmenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\grmenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\grmenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from config import TAB_CFG, TS_CFG\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# tabular helpers\n",
    "# -----------------------------\n",
    "def read_csv_light(path, usecols=None, dtype=None, parse_dates=None, chunksize=None):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        usecols=usecols,\n",
    "        dtype=dtype,\n",
    "        parse_dates=parse_dates,\n",
    "        low_memory=False,\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "def build_subject_index(df, key=\"subject_id\"):\n",
    "    return df.groupby(key, sort=False).indices\n",
    "\n",
    "def load_tabular_all(TAB_DIR: str):\n",
    "    tab_df = {}\n",
    "    tab_idx = {}\n",
    "\n",
    "    for name, cfg in TAB_CFG.items():\n",
    "        path = os.path.join(TAB_DIR, f\"{name}.csv\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[TAB][SKIP] {name}.csv not found\")\n",
    "            continue\n",
    "\n",
    "        df = read_csv_light(\n",
    "            path,\n",
    "            usecols=cfg.get(\"usecols\"),\n",
    "            dtype=cfg.get(\"dtype\"),\n",
    "            parse_dates=cfg.get(\"parse_dates\"),\n",
    "            chunksize=None,\n",
    "        )\n",
    "\n",
    "        if \"subject_id\" in df.columns:\n",
    "            df[\"subject_id\"] = pd.to_numeric(df[\"subject_id\"], errors=\"coerce\")\n",
    "            df = df[df[\"subject_id\"].notna()].copy()\n",
    "            df[\"subject_id\"] = df[\"subject_id\"].astype(\"int32\")\n",
    "\n",
    "        tab_df[name] = df\n",
    "        tab_idx[name] = build_subject_index(df, \"subject_id\") if \"subject_id\" in df.columns else {}\n",
    "        print(f\"[TAB] {name}: rows={len(df):,} cols={len(df.columns)}\")\n",
    "\n",
    "    return tab_df, tab_idx\n",
    "\n",
    "def query_tabular_indices(tab_idx, subject_id: int):\n",
    "    \"\"\"只返回每个表该 subject_id 对应的行号列表（更省）\"\"\"\n",
    "    sid = int(subject_id)\n",
    "    out = {}\n",
    "    for name, idx_map in tab_idx.items():\n",
    "        inds = idx_map.get(sid)\n",
    "        out[name] = inds.tolist() if inds is not None else []\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def iter_all_samples(\n",
    "    df_index: pd.DataFrame,\n",
    "    IMG_DIR: str,\n",
    "    tab_df, tab_idx,\n",
    "    text_map_dicom, text_map_subject,\n",
    "    ts_reader: TimeseriesBucketReader,\n",
    "    ts_tables,\n",
    "    hours: int = 48,\n",
    "):\n",
    "    \"\"\"\n",
    "    逐样本yield：每次只在内存里保留一个sample，适合全量不炸。\n",
    "    \"\"\"\n",
    "    for row in df_index.itertuples(index=False):\n",
    "        sample_id = getattr(row, \"sample_id\")\n",
    "        subject_id = int(getattr(row, \"subject_id\"))\n",
    "        dicom_id = getattr(row, \"dicom_id\", None)\n",
    "        cxr_time = pd.to_datetime(getattr(row, \"cxr_time\"), errors=\"coerce\")\n",
    "\n",
    "        sample = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"subject_id\": subject_id,\n",
    "            \"dicom_id\": dicom_id,\n",
    "            \"cxr_time\": cxr_time,\n",
    "\n",
    "            \"label_any\": getattr(row, \"label_any\", None),\n",
    "            \"label_multi\": getattr(row, \"label_multi\", None),\n",
    "\n",
    "            # meta如果你要也可以放：但别放太大字段\n",
    "            \"meta\": row._asdict(),\n",
    "        }\n",
    "\n",
    "        # -------- image：这里是“真的读出来” --------\n",
    "        img_path = os.path.join(IMG_DIR, f\"{sample_id}.npy\")\n",
    "        sample[\"image\"] = np.load(img_path) if os.path.exists(img_path) else None\n",
    "\n",
    "        # -------- text：真的读出来 --------\n",
    "        sample[\"text\"] = query_text(\n",
    "            text_map_dicom=text_map_dicom,\n",
    "            text_map_subject=text_map_subject,\n",
    "            subject_id=subject_id,\n",
    "            dicom_id=dicom_id,\n",
    "            cxr_time=cxr_time,\n",
    "            tol_minutes=5,\n",
    "        )\n",
    "\n",
    "        # -------- tabular：真的读出来（按subject过滤）--------\n",
    "        sample[\"tabular\"] = query_tabular(tab_df, tab_idx, subject_id)\n",
    "\n",
    "        # -------- timeseries：真的读出来（48h window）--------\n",
    "        ts_data = {}\n",
    "        for tname in ts_tables:\n",
    "            ts_data[tname] = ts_reader.query_window(\n",
    "                table=tname,\n",
    "                subject_id=subject_id,\n",
    "                cxr_time=cxr_time,\n",
    "                hours=hours,\n",
    "            )\n",
    "        sample[\"timeseries\"] = ts_data\n",
    "\n",
    "        yield sample\n",
    "\n",
    "\n",
    "def process_one(sample: dict):\n",
    "    \"\"\"\n",
    "    你下一步处理写在这里：比如提特征、做聚合、保存到磁盘、喂模型等。\n",
    "    这里给个占位示例：统计各表行数。\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"sample_id\": sample[\"sample_id\"],\n",
    "        \"ts_lens\": {k: len(v) for k, v in sample[\"timeseries\"].items()},\n",
    "        \"text_len\": len(sample[\"text\"]),\n",
    "        \"has_image\": sample[\"image\"] is not None,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ROOT = r\"E:/NUS/data/perdata/train_text_all_samples\"\n",
    "    META_DIR = os.path.join(ROOT, \"meta\")\n",
    "    IMG_DIR = os.path.join(ROOT, \"images\")\n",
    "    TAB_DIR = os.path.join(ROOT, \"tabular\")\n",
    "    TEXT_PATH = os.path.join(ROOT, \"text.csv\")\n",
    "\n",
    "    TS_BUCKET_ROOT = r\"E:/NUS/data/perdata/train_text_all_samples/timeseries_bucketed\"\n",
    "    SAMPLE_INDEX_PATH = os.path.join(META_DIR, \"sample_index.json\")\n",
    "\n",
    "    with open(SAMPLE_INDEX_PATH, \"r\") as f:\n",
    "        sample_index = json.load(f)\n",
    "    df_index = pd.DataFrame(sample_index)\n",
    "    print(\"Total samples:\", len(df_index))\n",
    "\n",
    "    tab_df, tab_idx = load_tabular_all(TAB_DIR)\n",
    "    _, text_map_dicom, text_map_subject = load_text_table(TEXT_PATH)\n",
    "\n",
    "    ts_reader = TimeseriesBucketReader(\n",
    "        bucket_root=TS_BUCKET_ROOT,\n",
    "        num_buckets=128,   # 这里要和你 bucketize 一致\n",
    "        cache_size=4,\n",
    "    )\n",
    "    ts_tables = list(TS_CFG.keys())\n",
    "\n",
    "    # ===== 分开存四部分 =====\n",
    "    metas = []        # 只存必要meta（小）\n",
    "    imgs  = []        # numpy array / None\n",
    "    texts = []        # str\n",
    "    tabs  = []        # dict(table_name -> df)\n",
    "    tss   = []        # dict(ts_table -> df)\n",
    "\n",
    "    for i, sample in enumerate(iter_all_samples(\n",
    "        df_index=df_index,\n",
    "        IMG_DIR=IMG_DIR,\n",
    "        tab_df=tab_df,\n",
    "        tab_idx=tab_idx,\n",
    "        text_map_dicom=text_map_dicom,\n",
    "        text_map_subject=text_map_subject,\n",
    "        ts_reader=ts_reader,\n",
    "        ts_tables=ts_tables,\n",
    "        hours=48,\n",
    "    )):\n",
    "        # meta：建议只留你要的字段，别把 row._asdict() 全塞进去也行\n",
    "        metas.append({\n",
    "            \"sample_id\": sample[\"sample_id\"],\n",
    "            \"subject_id\": sample[\"subject_id\"],\n",
    "            \"dicom_id\": sample.get(\"dicom_id\"),\n",
    "            \"cxr_time\": sample.get(\"cxr_time\"),\n",
    "            \"label_any\": sample.get(\"label_any\"),\n",
    "            \"label_multi\": sample.get(\"label_multi\"),\n",
    "        })\n",
    "\n",
    "        imgs.append(sample[\"image\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "        tabs.append(sample[\"tabular\"])\n",
    "        tss.append(sample[\"timeseries\"])\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"[PROGRESS] {i+1:,}/{len(df_index):,}\")\n",
    "\n",
    "    print(\"done\")\n",
    "    print(\"metas:\", len(metas), \"imgs:\", len(imgs), \"texts:\", len(texts), \"tabs:\", len(tabs), \"tss:\", len(tss))\n",
    "\n",
    "    # ===== 你可以马上抽查第0条 =====\n",
    "    k = 0\n",
    "    print(\"sample_id:\", metas[k][\"sample_id\"])\n",
    "    print(\"img shape:\", None if imgs[k] is None else imgs[k].shape)\n",
    "    print(\"text len:\", len(texts[k]))\n",
    "    print(\"tab keys:\", list(tabs[k].keys()))\n",
    "    print(\"ts keys:\", list(tss[k].keys()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92078b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sid: 10000032\n",
      "reader.num_buckets: 8\n",
      "expected bucket path: E:/NUS/data/perdata/train_text_samples/timeseries_bucketed\\chartevents\\bucket_000.csv exists: True\n",
      "bucket rows: 90619 hit: 0\n"
     ]
    }
   ],
   "source": [
    "sid = int(s[\"subject_id\"])\n",
    "print(\"sid:\", sid)\n",
    "\n",
    "# 1) 你 reader 用的 buckets 数\n",
    "print(\"reader.num_buckets:\", ts_reader.num_buckets)\n",
    "\n",
    "# 2) bucket 文件是否存在（你算出来的 bucket）\n",
    "bid = sid % ts_reader.num_buckets\n",
    "path = os.path.join(ts_reader.bucket_root, \"chartevents\", f\"bucket_{bid:03d}.csv\")\n",
    "print(\"expected bucket path:\", path, \"exists:\", os.path.exists(path))\n",
    "\n",
    "# 3) 如果存在，看看这个 bucket 里到底有没有 sid\n",
    "if os.path.exists(path):\n",
    "    df0 = pd.read_csv(path, usecols=[\"subject_id\"], low_memory=False)\n",
    "    col = pd.to_numeric(df0[\"subject_id\"], errors=\"coerce\")\n",
    "    print(\"bucket rows:\", len(col), \"hit:\", (col == sid).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c75f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found sid in SOURCE chartevents: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sid = 10000032\n",
    "src = r\"E:/NUS/data/perdata/train_text_samples/timeseries_merged/chartevents.csv\"\n",
    "\n",
    "found = False\n",
    "for chunk in pd.read_csv(src, usecols=[\"subject_id\"], chunksize=200000, low_memory=False):\n",
    "    col = pd.to_numeric(chunk[\"subject_id\"], errors=\"coerce\")\n",
    "    if (col == sid).any():\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "print(\"found sid in SOURCE chartevents:\", found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5c38a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_text_table' from 'dataset' (f:\\study\\NUS\\capstone\\GMM\\data\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TAB_CFG, TS_CFG\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tabular_all, TimeseriesBucketReader, load_text_table, query_text, query_tabular\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miter_all_samples\u001b[39m(\n\u001b[0;32m     13\u001b[0m     df_index: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m     14\u001b[0m     IMG_DIR: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     hours: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m48\u001b[39m,\n\u001b[0;32m     20\u001b[0m ):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    逐样本yield：每次只在内存里保留一个sample，适合全量不炸。\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_text_table' from 'dataset' (f:\\study\\NUS\\capstone\\GMM\\data\\dataset.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from config import TAB_CFG, TS_CFG\n",
    "\n",
    "from dataset import load_tabular_all, TimeseriesBucketReader, load_text_table, query_text, query_tabular\n",
    "\n",
    "\n",
    "def iter_all_samples(\n",
    "    df_index: pd.DataFrame,\n",
    "    IMG_DIR: str,\n",
    "    tab_df, tab_idx,\n",
    "    text_map_dicom, text_map_subject,\n",
    "    ts_reader: TimeseriesBucketReader,\n",
    "    ts_tables,\n",
    "    hours: int = 48,\n",
    "):\n",
    "    \"\"\"\n",
    "    逐样本yield：每次只在内存里保留一个sample，适合全量不炸。\n",
    "    \"\"\"\n",
    "    for row in df_index.itertuples(index=False):\n",
    "        sample_id = getattr(row, \"sample_id\")\n",
    "        subject_id = int(getattr(row, \"subject_id\"))\n",
    "        dicom_id = getattr(row, \"dicom_id\", None)\n",
    "        cxr_time = pd.to_datetime(getattr(row, \"cxr_time\"), errors=\"coerce\")\n",
    "\n",
    "        sample = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"subject_id\": subject_id,\n",
    "            \"dicom_id\": dicom_id,\n",
    "            \"cxr_time\": cxr_time,\n",
    "\n",
    "            \"label_any\": getattr(row, \"label_any\", None),\n",
    "            \"label_multi\": getattr(row, \"label_multi\", None),\n",
    "\n",
    "            # meta如果你要也可以放：但别放太大字段\n",
    "            \"meta\": row._asdict(),\n",
    "        }\n",
    "\n",
    "        # -------- image：这里是“真的读出来” --------\n",
    "        img_path = os.path.join(IMG_DIR, f\"{sample_id}.npy\")\n",
    "        sample[\"image\"] = np.load(img_path) if os.path.exists(img_path) else None\n",
    "\n",
    "        # -------- text：真的读出来 --------\n",
    "        sample[\"text\"] = query_text(\n",
    "            text_map_dicom=text_map_dicom,\n",
    "            text_map_subject=text_map_subject,\n",
    "            subject_id=subject_id,\n",
    "            dicom_id=dicom_id,\n",
    "            cxr_time=cxr_time,\n",
    "            tol_minutes=5,\n",
    "        )\n",
    "\n",
    "        # -------- tabular：真的读出来（按subject过滤）--------\n",
    "        sample[\"tabular\"] = query_tabular(tab_df, tab_idx, subject_id)\n",
    "\n",
    "        # -------- timeseries：真的读出来（48h window）--------\n",
    "        ts_data = {}\n",
    "        for tname in ts_tables:\n",
    "            ts_data[tname] = ts_reader.query_window(\n",
    "                table=tname,\n",
    "                subject_id=subject_id,\n",
    "                cxr_time=cxr_time,\n",
    "                hours=hours,\n",
    "            )\n",
    "        sample[\"timeseries\"] = ts_data\n",
    "\n",
    "        yield sample\n",
    "\n",
    "\n",
    "def process_one(sample: dict):\n",
    "    \"\"\"\n",
    "    你下一步处理写在这里：比如提特征、做聚合、保存到磁盘、喂模型等。\n",
    "    这里给个占位示例：统计各表行数。\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"sample_id\": sample[\"sample_id\"],\n",
    "        \"ts_lens\": {k: len(v) for k, v in sample[\"timeseries\"].items()},\n",
    "        \"text_len\": len(sample[\"text\"]),\n",
    "        \"has_image\": sample[\"image\"] is not None,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ROOT = r\"E:/NUS/data/perdata/train_text_samples\"\n",
    "    META_DIR = os.path.join(ROOT, \"meta\")\n",
    "    IMG_DIR = os.path.join(ROOT, \"images\")\n",
    "    TAB_DIR = os.path.join(ROOT, \"tabular\")\n",
    "    TEXT_PATH = os.path.join(ROOT, \"text.csv\")\n",
    "\n",
    "    TS_BUCKET_ROOT = r\"E:/NUS/data/perdata/timeseries_bucketed\"\n",
    "    SAMPLE_INDEX_PATH = os.path.join(META_DIR, \"sample_index.json\")\n",
    "\n",
    "    with open(SAMPLE_INDEX_PATH, \"r\") as f:\n",
    "        sample_index = json.load(f)\n",
    "    df_index = pd.DataFrame(sample_index)\n",
    "    print(\"Total samples:\", len(df_index))\n",
    "\n",
    "    # load once\n",
    "    tab_df, tab_idx = load_tabular_all(TAB_DIR)\n",
    "    _, text_map_dicom, text_map_subject = load_text_table(TEXT_PATH)\n",
    "\n",
    "    # ✅ 必须和 bucketize 一致\n",
    "    ts_reader = TimeseriesBucketReader(\n",
    "        bucket_root=TS_BUCKET_ROOT,\n",
    "        num_buckets=128,\n",
    "        cache_size=6,\n",
    "    )\n",
    "    ts_tables = list(TS_CFG.keys())\n",
    "\n",
    "    # 全量 streaming 处理（不存list）\n",
    "    results = []\n",
    "    for i, sample in enumerate(iter_all_samples(\n",
    "        df_index=df_index,\n",
    "        IMG_DIR=IMG_DIR,\n",
    "        tab_df=tab_df,\n",
    "        tab_idx=tab_idx,\n",
    "        text_map_dicom=text_map_dicom,\n",
    "        text_map_subject=text_map_subject,\n",
    "        ts_reader=ts_reader,\n",
    "        ts_tables=ts_tables,\n",
    "        hours=48,\n",
    "    )):\n",
    "        results.append(sample)\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"[PROGRESS] {i+1:,}/{len(df_index):,}\")\n",
    "\n",
    "    print(\"done, results:\", len(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "683cbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: ((224, 224), dtype('uint8'))\n",
      "text first 200: \n",
      "tabular keys: ['demographics', 'admissions', 'icustays', 'transfers', 'prescriptions', 'procedures_icd']\n",
      "admissions head:\n",
      "   subject_id   hadm_id           admittime admission_type admission_location  \\\n",
      "0    10000032  29079034 2180-07-23 12:35:00       EW EMER.     EMERGENCY ROOM   \n",
      "1    10000032  29079034 2180-07-23 12:35:00       EW EMER.     EMERGENCY ROOM   \n",
      "2    10000032  29079034 2180-07-23 12:35:00       EW EMER.     EMERGENCY ROOM   \n",
      "3    10000032  29079034 2180-07-23 12:35:00       EW EMER.     EMERGENCY ROOM   \n",
      "4    10000032  29079034 2180-07-23 12:35:00       EW EMER.     EMERGENCY ROOM   \n",
      "\n",
      "  insurance language marital_status  \n",
      "0  Medicaid  ENGLISH        WIDOWED  \n",
      "1  Medicaid  ENGLISH        WIDOWED  \n",
      "2  Medicaid  ENGLISH        WIDOWED  \n",
      "3  Medicaid  ENGLISH        WIDOWED  \n",
      "4  Medicaid  ENGLISH        WIDOWED  \n",
      "timeseries keys: ['chartevents', 'labevents', 'inputevents', 'outputevents', 'procedureevents']\n",
      "chartevents head:\n",
      "Empty DataFrame\n",
      "Columns: [subject_id, hadm_id, stay_id, charttime, itemid, valuenum, valueuom]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "s = results[0]   # 或者你当前 sample\n",
    "\n",
    "print(\"image:\", None if s[\"image\"] is None else (s[\"image\"].shape, s[\"image\"].dtype))\n",
    "print(\"text first 200:\", s[\"text\"][:200])\n",
    "\n",
    "print(\"tabular keys:\", list(s[\"tabular\"].keys()))\n",
    "print(\"admissions head:\")\n",
    "print(s[\"tabular\"][\"admissions\"].head())\n",
    "\n",
    "print(\"timeseries keys:\", list(s[\"timeseries\"].keys()))\n",
    "print(\"chartevents head:\")\n",
    "print(s[\"timeseries\"][\"chartevents\"].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a97cb835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id: s_000000\n",
      "image: ((224, 224), dtype('uint8'))\n",
      "text_len: 0\n",
      "tabular rows:\n",
      "  demographics 10\n",
      "  admissions 10\n",
      "  icustays 10\n",
      "  transfers 60\n",
      "  prescriptions 240\n",
      "  procedures_icd 0\n",
      "timeseries rows (window):\n",
      "  chartevents 0\n",
      "  labevents 0\n",
      "  inputevents 0\n",
      "  outputevents 0\n",
      "  procedureevents 0\n"
     ]
    }
   ],
   "source": [
    "   # 拿第一个 sample 或你指定的 sample\n",
    "\n",
    "print(\"sample_id:\", s[\"sample_id\"])\n",
    "print(\"image:\", None if s[\"image\"] is None else (s[\"image\"].shape, s[\"image\"].dtype))\n",
    "print(\"text_len:\", len(s[\"text\"]))\n",
    "\n",
    "print(\"tabular rows:\")\n",
    "for k, df in s[\"tabular\"].items():\n",
    "    print(\" \", k, len(df))\n",
    "\n",
    "print(\"timeseries rows (window):\")\n",
    "for k, df in s[\"timeseries\"].items():\n",
    "    print(\" \", k, len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3b486df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_id = \"s_000000\"\n",
    "src = r\"E:/NUS/data/perdata/train_text_samples/timeseries_merged/chartevents.csv\"\n",
    "\n",
    "hit = []\n",
    "for chunk in pd.read_csv(src, chunksize=200000, low_memory=False):\n",
    "    sub = chunk[chunk[\"sample_id\"] == sample_id]\n",
    "    if len(sub):\n",
    "        hit.append(sub)\n",
    "\n",
    "df = pd.concat(hit, ignore_index=True) if hit else pd.DataFrame()\n",
    "print(\"rows:\", len(df))\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e11e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputevents 数量: 1778\n",
      "demographics 数量: 2990\n",
      "只在 outputevents: 0\n",
      "只在 demographics: 1212\n",
      "是否完全一致: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读数据\n",
    "outputevents = pd.read_csv(\n",
    "    r\"E:\\NUS\\data\\perdata\\train_text_all_samples\\timeseries_merged\\outputevents.csv\"\n",
    ")\n",
    "demographics = pd.read_csv(\n",
    "    r\"E:\\NUS\\data\\perdata\\train_text_all_samples\\tabular\\demographics.csv\"\n",
    ")\n",
    "\n",
    "# 取 subject_id 集合\n",
    "oe_ids = set(outputevents['subject_id'].dropna().unique())\n",
    "demo_ids = set(demographics['subject_id'].dropna().unique())\n",
    "\n",
    "print(\"outputevents 数量:\", len(oe_ids))\n",
    "print(\"demographics 数量:\", len(demo_ids))\n",
    "print(\"只在 outputevents:\", len(oe_ids - demo_ids))\n",
    "print(\"只在 demographics:\", len(demo_ids - oe_ids))\n",
    "print(\"是否完全一致:\", oe_ids == demo_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
