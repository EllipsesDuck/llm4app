{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c5df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"satyajeetrai/medical-cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cb9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lixiaokang/.cache/kagglehub/datasets/satyajeetrai/medical-cost/versions/1\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94e289eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca858f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b17acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/torch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m118.5 kB/s\u001b[0m  \u001b[33m0:01:16\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "Successfully installed pandas-2.3.3\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y pandas\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05440a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['home', 'usr', '.resolve', 'bin', 'sbin', '.file', 'etc', 'var', 'Library', 'System', '.VolumeIcon.icns', 'private', '.vol', 'Users', 'Applications', 'opt', 'dev', 'Volumes', '.nofollow', 'tmp', 'cores']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, Optional, List\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"MIMIC-DataLoader\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FileSpec:\n",
    "    \"\"\"File specification: name + dtype overrides\"\"\"\n",
    "    filename: str\n",
    "    dtypes: Optional[Dict[str, str]] = None\n",
    "\n",
    "class MIMICDataLoader:\n",
    "    \"\"\"\n",
    "    Elegant, extensible DataLoader for MIMIC-III/IV.\n",
    "    Features:\n",
    "    - automatic dtype handling\n",
    "    - clean logging\n",
    "    - modular structure\n",
    "    - simple to extend to new tables\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.datasets: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "        # Required core files\n",
    "        self.required_files: List[FileSpec] = [\n",
    "            FileSpec(\"DIAGNOSES_ICD.csv\", {\"icd9_code\": str}),\n",
    "            FileSpec(\"PATIENTS.csv\"),\n",
    "            FileSpec(\"PRESCRIPTIONS.csv\", {\"drug_name_generic\": str}),\n",
    "            FileSpec(\"LABEVENTS.csv\"),\n",
    "            FileSpec(\"CHARTEVENTS.csv\"),\n",
    "            FileSpec(\"ICUSTAYS.csv\"),\n",
    "            FileSpec(\"MICROBIOLOGYEVENTS.csv\", {\"org_itemid\": str, \"ab_itemid\": str}),\n",
    "        ]\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _filepath(self, filename):\n",
    "        return os.path.join(self.data_path, filename)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _load_one(self, spec: FileSpec):\n",
    "        \"\"\"Load a single MIMIC file with clean logging + dtype override.\"\"\"\n",
    "        path = self._filepath(spec.filename)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            logger.warning(f\"[SKIP] File not found: {spec.filename}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=spec.dtypes, low_memory=False)\n",
    "            logger.info(f\"[OK] Loaded {spec.filename:<25} shape={df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] Failed to load {spec.filename}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    # ----------------------------------------------------------------------\n",
    "    def load_all(self):\n",
    "        \"\"\"Load all required files into self.datasets.\"\"\"\n",
    "        for spec in self.required_files:\n",
    "            key = spec.filename.split(\".\")[0].lower()\n",
    "            self.datasets[key] = self._load_one(spec)\n",
    "\n",
    "        self._validate()\n",
    "        return self.datasets\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _validate(self):\n",
    "        \"\"\"Ensure that critical datasets exist and are non-empty.\"\"\"\n",
    "        mandatory = [\"diagnoses_icd\", \"patients\", \"prescriptions\"]\n",
    "\n",
    "        for key in mandatory:\n",
    "            df = self.datasets.get(key)\n",
    "            if df is None or df.empty:\n",
    "                raise ValueError(f\"[ERROR] Required dataset missing or empty: {key}\")\n",
    "\n",
    "        logger.info(\"[OK] All required datasets successfully validated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f42a608",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32mpandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/missing.pyx:40\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_path = os.path.join(path, \"dataset_.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97949b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([4, 4, 5000])\n",
      "\\n[Stage 1] Supervised CE Training:\n",
      "step 0: CE loss = 8.6934\n",
      "step 1: CE loss = 6.8178\n",
      "step 2: CE loss = 5.0920\n",
      "\\n[Stage 2] GBPO Reinforcement Fine-tune:\n",
      "step 0: RL loss = 3.6789\n",
      "step 1: RL loss = 2.5219\n",
      "step 2: RL loss = 1.5357\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional, Dict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    y = x * weight / sqrt(mean(x^2) + eps)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim)\n",
    "        norm = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(norm + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "\n",
    "def _split_heads(x, num_heads):\n",
    "    \"\"\"将最后一维分解为多头：(B, T, H*d) -> (B, T, H, d)\n",
    "    \"\"\"\n",
    "    B, T, D = x.shape\n",
    "    assert D % num_heads == 0, f\"Hidden size {D} not divisible by heads {num_heads}\"\n",
    "    d = D // num_heads\n",
    "    return x.view(B, T, num_heads, d)\n",
    "\n",
    "\n",
    "def _merge_heads(x):\n",
    "    \"\"\"(B, T, H, d) -> (B, T, H*d)\n",
    "    \"\"\"\n",
    "    B, T, H, d = x.shape\n",
    "    return x.contiguous().view(B, T, H * d)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,attn_mask,dropout_p=0.0,training=True):\n",
    "    \"\"\"基础 SDPA\n",
    "      q: (B, T_q, H_q, d)\n",
    "      k: (B, T_k, H_k, d)\n",
    "      v: (B, T_k, H_k, d)\n",
    "      attn_mask: (B, 1, T_q, T_k) or (1, 1, T_q, T_k)，-inf 位置被屏蔽\n",
    "    \"\"\"\n",
    "    d = q.size(-1)\n",
    "    # (B, H_q, T_q, d) @ (B, H_k, d, T_k) → (B, H_q, T_q, T_k)\n",
    "    q_ = q.permute(0, 2, 1, 3)\n",
    "    k_ = k.permute(0, 2, 3, 1)\n",
    "    attn_scores = torch.matmul(q_, k_) / math.sqrt(d)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        attn_scores = attn_scores + attn_mask  # 预期 mask 已为 -inf/0 形式\n",
    "\n",
    "    attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "    if dropout_p > 0 and training:\n",
    "        attn_probs = F.dropout(attn_probs, p=dropout_p)\n",
    "\n",
    "    # (B, H_q, T_q, T_k) @ (B, H_k, T_k, d) → (B, H_q, T_q, d)\n",
    "    v_ = v.permute(0, 2, 1, 3)\n",
    "    context = torch.matmul(attn_probs, v_)\n",
    "    # -> (B, T_q, H_q, d)\n",
    "    context = context.permute(0, 2, 1, 3)\n",
    "    return context, attn_probs\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"标准多头自注意力（支持可选的 causal mask）。\"\"\"\n",
    "    def __init__(self, d_model, n_heads, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_drop = attn_drop\n",
    "        self.proj_drop = proj_drop\n",
    "        self.norm_qkv = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # x: (B, T, D)\n",
    "        B, T, D = x.shape\n",
    "        x = self.norm_qkv(x)\n",
    "        qkv = self.qkv(x)  # (B, T, 3D)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = _split_heads(q, self.n_heads)\n",
    "        k = _split_heads(k, self.n_heads)\n",
    "        v = _split_heads(v, self.n_heads)\n",
    "\n",
    "        # causal mask: (1, 1, T, T)\n",
    "        attn_mask = None\n",
    "        if causal:\n",
    "            # 下三角为0，上三角为 -inf\n",
    "            mask = torch.full((T, T), float('-inf'), device=x.device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            attn_mask = mask.unsqueeze(0).unsqueeze(0)  # (1,1,T,T)\n",
    "\n",
    "        ctx, _ = scaled_dot_product_attention(q, k, v, attn_mask=attn_mask,\n",
    "                                              dropout_p=self.attn_drop, training=self.training)\n",
    "        out = _merge_heads(ctx)  # (B, T, D)\n",
    "        out = self.out_proj(out)\n",
    "        if self.proj_drop > 0:\n",
    "            out = F.dropout(out, p=self.proj_drop, training=self.training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LazyCrossAttentionGQA(nn.Module):\n",
    "    \"\"\"\n",
    "    懒惰交叉注意力（GQA 版本）：\n",
    "      - 仅对 Query 做线性映射；K/V 由 ContextProcessor 直接提供（已按 Gkv 分组，无需 Wk/Wv）\n",
    "      - 支持 H_q 查询头数量与 G_kv 组数不同（H_q >= G_kv），通过 repeat_interleave 将 (K,V) 组扩展到 H_q\n",
    "    输入：\n",
    "      x_q: (B, T_q, D)\n",
    "      k_ctx, v_ctx: (B, T_k, Gkv, d_head)\n",
    "    参数：\n",
    "      n_heads_q = H_q, d_head = D // H_q\n",
    "    输出：\n",
    "      (B, T_q, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads_q, gkv, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert n_heads_q % gkv == 0 # n_heads_q 必须能被 Gkv 整除（每组共享一份 K/V）\n",
    "        self.d_model = d_model\n",
    "        self.n_heads_q = n_heads_q\n",
    "        self.gkv = gkv\n",
    "        self.d_head = d_model // n_heads_q\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_drop = attn_drop\n",
    "        self.proj_drop = proj_drop\n",
    "        self.norm_q = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self,x_q,k_ctx,v_ctx,attn_mask=None):\n",
    "        # x_q: (B, T_q, D); k_ctx/v_ctx: (B, T_k, Gkv, d_head)\n",
    "        B, Tq, D = x_q.shape\n",
    "        _, Tk, Gkv, d = k_ctx.shape\n",
    "        assert Gkv == self.gkv and d == self.d_head\n",
    "\n",
    "        q = self.q_proj(self.norm_q(x_q))  # (B, Tq, D)\n",
    "        q = _split_heads(q, self.n_heads_q)  # (B, Tq, Hq, d)\n",
    "\n",
    "        # 将 (B, Tk, Gkv, d) 映射为 (B, Tk, Hq, d)，通过 repeat_interleave\n",
    "        repeat = self.n_heads_q // self.gkv\n",
    "        k = k_ctx.repeat_interleave(repeat, dim=2)\n",
    "        v = v_ctx.repeat_interleave(repeat, dim=2)\n",
    "\n",
    "        ctx, _ = scaled_dot_product_attention(q, k, v, attn_mask=attn_mask,\n",
    "                                              dropout_p=self.attn_drop, training=self.training)\n",
    "        out = _merge_heads(ctx)  # (B, Tq, D)\n",
    "        out = self.out_proj(out)\n",
    "        if self.proj_drop > 0:\n",
    "            out = F.dropout(out, p=self.proj_drop, training=self.training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop=0.0, activation=\"silu\"):\n",
    "        super().__init__()\n",
    "        act = nn.SiLU() if activation == \"silu\" else nn.GELU()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            act,\n",
    "            nn.Linear(d_ff, d_model, bias=False),\n",
    "        )\n",
    "        self.drop = drop\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.norm(x)\n",
    "        y = self.net(x_in)\n",
    "        if self.drop > 0:\n",
    "            y = F.dropout(y, p=self.drop, training=self.training)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_experts=8, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.drop = drop\n",
    "\n",
    "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(d_ff, d_model),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # -------- gating --------\n",
    "        gate_logits = self.gate(x_norm)         # (B,T,E)\n",
    "        gate_scores = torch.softmax(gate_logits, dim=-1)\n",
    "        top1_idx = gate_scores.argmax(dim=-1)   # (B,T)\n",
    "        top1_score = gate_scores.max(dim=-1).values  # (B,T)\n",
    "\n",
    "        # -------- output buffer --------\n",
    "        out = torch.zeros_like(x_norm)\n",
    "\n",
    "        # -------- per expert routing --------\n",
    "        for e in range(self.num_experts):\n",
    "            # mask: (B,T)\n",
    "            mask = (top1_idx == e)\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # 取出所有属于 expert e 的 token → (Ne, D)\n",
    "            xe = x_norm[mask]       # 直接在 2D flatten 上 mask\n",
    "\n",
    "            # expert forward\n",
    "            ye = self.experts[e](xe)   # (Ne, D)\n",
    "\n",
    "            # 写回对应 token\n",
    "            out[mask] = ye\n",
    "\n",
    "        # gating score 缩放\n",
    "        out = out * top1_score.unsqueeze(-1)\n",
    "\n",
    "        if self.drop > 0:\n",
    "            out = F.dropout(out, p=self.drop, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ContextProcessor(nn.Module):\n",
    "    \"\"\"\n",
    "    将多路上下文输入（用户静态、短期、长期）统一映射到 (k_l, v_l) 列表：\n",
    "      - d_context = Skv * Lkv * Gkv * d_head\n",
    "      - 将特征维度切成 Lkv 份（每份包含 Skv 个槽），得到每层的 (k_l, v_l)\n",
    "      - 当 Skv=1：v_l = k_l（KV 共享）；当 Skv=2：第 0 槽为 k_l，第 1 槽为 v_l\n",
    "    期望输入：\n",
    "      user_static:  (B, Ns, D_in)\n",
    "      short_term:   (B, Ts, D_in)\n",
    "      long_term:    (B, Tl, D_in)\n",
    "    输出：\n",
    "      kv_list: List[(k_l, v_l)], 其中 k_l/v_l -> (B, T_ctx, Gkv, d_head)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_in,\n",
    "                 d_head,\n",
    "                 gkv,\n",
    "                 lkv=1,\n",
    "                 skv=1,\n",
    "                 use_norm_k=True,\n",
    "                 use_norm_v=True):\n",
    "        super().__init__()\n",
    "        assert skv in (1, 2)\n",
    "        self.d_in = d_in\n",
    "        self.d_head = d_head\n",
    "        self.gkv = gkv\n",
    "        self.lkv = lkv\n",
    "        self.skv = skv\n",
    "        self.d_context = skv * lkv * gkv * d_head\n",
    "\n",
    "        # 三路线性映射到相同维度后拼接（也可换成各自独立映射 + concat + 再线性）\n",
    "        self.proj = nn.Linear(d_in, self.d_context, bias=False)\n",
    "        self.norm_k_layers = nn.ModuleList([RMSNorm(gkv * d_head) if use_norm_k else nn.Identity()\n",
    "                                            for _ in range(lkv)])\n",
    "        self.norm_v_layers = nn.ModuleList([RMSNorm(gkv * d_head) if use_norm_v else nn.Identity()\n",
    "                                            for _ in range(lkv)])\n",
    "\n",
    "    def forward(self,user_static,short_term,long_term):\n",
    "        ctx_parts = []\n",
    "        for x in (user_static, short_term, long_term):\n",
    "            if x is not None:\n",
    "                # x: (B, T, D_in) → (B, T, d_context)\n",
    "                ctx_parts.append(self.proj(x))\n",
    "        assert len(ctx_parts) > 0 # 至少需要一条上下文输入\n",
    "        # 沿时间维拼接： (B, T_ctx, d_context)\n",
    "        ctx = torch.cat(ctx_parts, dim=1) if len(ctx_parts) > 1 else ctx_parts[0]\n",
    "\n",
    "        B, Tctx, D = ctx.shape\n",
    "        assert D == self.d_context\n",
    "\n",
    "        # 切块：Lkv 份，每份大小 = skv * gkv * d_head\n",
    "        chunk_size = self.skv * self.gkv * self.d_head\n",
    "        chunks = ctx.split(chunk_size, dim=-1)  # 长度应为 Lkv\n",
    "        assert len(chunks) == self.lkv, f\"期望 {self.lkv} 份，得到 {len(chunks)}\"\n",
    "\n",
    "        kv_list = []\n",
    "        for l, ch in enumerate(chunks):\n",
    "            # ch: (B, Tctx, skv*gkv*d_head)\n",
    "            if self.skv == 1:\n",
    "                # 共享：v_l = k_l\n",
    "                k = ch  # (B, Tctx, gkv*d_head)\n",
    "                k = self.norm_k_layers[l](k)\n",
    "                # reshape 到 (B,T,Gkv,d_head)\n",
    "                k = k.view(B, Tctx, self.gkv, self.d_head)\n",
    "                v = k\n",
    "            else:\n",
    "                # 独立：前半为 K，后半为 V\n",
    "                mid = (self.gkv * self.d_head)\n",
    "                k, v = ch[..., :mid], ch[..., mid:]\n",
    "                k = self.norm_k_layers[l](k)\n",
    "                v = self.norm_v_layers[l](v)\n",
    "                k = k.view(B, Tctx, self.gkv, self.d_head)\n",
    "                v = v.view(B, Tctx, self.gkv, self.d_head)\n",
    "            kv_list.append((k, v))\n",
    "        return kv_list\n",
    "\n",
    "\n",
    "class LazyDecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 n_heads_q,\n",
    "                 gkv,\n",
    "                 d_ff,\n",
    "                 attn_drop=0.0,\n",
    "                 resid_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.cross_attn = LazyCrossAttentionGQA(d_model, n_heads_q, gkv,\n",
    "                                               attn_drop=attn_drop, proj_drop=resid_drop)\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, n_heads_q,\n",
    "                                               attn_drop=attn_drop, proj_drop=resid_drop)\n",
    "\n",
    "        # ⭐ FFN → MoE\n",
    "        self.ffn = MoEFeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            num_experts=8,     # 或者 54，完全看你规模要求\n",
    "            drop=resid_drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, k_ctx, v_ctx, causal=True):\n",
    "        x = x + self.cross_attn(x, k_ctx, v_ctx, attn_mask=None)\n",
    "        x = x + self.self_attn(x, causal=causal)\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LazyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    vocab_size：语义 ID 词表大小（例如 3 个 token 的共享词表）\n",
    "    d_model：主通道维度（= n_heads_q * d_head）\n",
    "    n_layers：解码层数 Nlayer\n",
    "    n_heads_q：解码端 query 头数 Hq\n",
    "    gkv：上下文 KV 组数（Gkv），需整除 Hq\n",
    "    d_ff：前馈隐层维度\n",
    "\n",
    "    Lkv：KV 层共享组数\n",
    "    Skv：1 表示 v=k（共享表示），2 表示独立 K/V\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 d_model = 768,\n",
    "                 n_layers = 12,\n",
    "                 n_heads_q = 12,\n",
    "                 gkv = 3,\n",
    "                 d_ff = 2048,\n",
    "                 # Context Processor\n",
    "                 d_ctx_in = 256,\n",
    "                 lkv = 1,\n",
    "                 skv = 1,\n",
    "                 pad_id = 0,\n",
    "                 bos_id = 1,\n",
    "                 attn_drop = 0.0,\n",
    "                 resid_drop = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads_q == 0\n",
    "        assert n_heads_q % gkv == 0\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads_q = n_heads_q\n",
    "        self.gkv = gkv\n",
    "        self.d_head = d_model // n_heads_q\n",
    "        self.pad_id = pad_id\n",
    "        self.bos_id = bos_id\n",
    "        self.lkv = lkv\n",
    "        self.skv = skv\n",
    "\n",
    "        # 嵌入层（共享）\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.out_norm = RMSNorm(d_model)\n",
    "        self.out_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # 上下文处理器：将输入特征映射为 (k_l, v_l) 列表\n",
    "        self.ctx_proc = ContextProcessor(\n",
    "            d_in=d_ctx_in,\n",
    "            d_head=self.d_head,\n",
    "            gkv=gkv,\n",
    "            lkv=lkv,\n",
    "            skv=skv,\n",
    "        )\n",
    "\n",
    "        # 构建 N 层解码块\n",
    "        self.blocks = nn.ModuleList([\n",
    "            LazyDecoderBlock(d_model, n_heads_q, gkv, d_ff,\n",
    "                             attn_drop=attn_drop, resid_drop=resid_drop)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def _kv_index_for_layer(self, l):\n",
    "        # l ∈ [0, Nlayer-1] → l_kv ∈ [0, Lkv-1]\n",
    "        return (l * self.lkv) // self.n_layers\n",
    "\n",
    "    def forward(self,\n",
    "                target_ids,  # (B, T_gen)，例如 [BOS, s1, s2]\n",
    "                user_static,  # (B, Ns, d_ctx_in)\n",
    "                short_term,  # (B, Ts, d_ctx_in)\n",
    "                long_term,   # (B, Tl, d_ctx_in)\n",
    "                return_hidden = False):\n",
    "        # 1) 生成 (k_l, v_l) 列表（按层共享）\n",
    "        kv_list = self.ctx_proc(user_static, short_term, long_term)\n",
    "\n",
    "        # 2) 目标 token 嵌入\n",
    "        x = self.tok_emb(target_ids)  # (B, T, D)\n",
    "\n",
    "        # 3) 逐层堆叠（Cross-Attn 使用共享 KV）\n",
    "        for l, blk in enumerate(self.blocks):\n",
    "            idx = self._kv_index_for_layer(l)\n",
    "            k_ctx, v_ctx = kv_list[idx]\n",
    "            x = blk(x, k_ctx, v_ctx, causal=True)\n",
    "\n",
    "        # 4) 输出分类头\n",
    "        h = self.out_norm(x)\n",
    "        logits = self.out_proj(h)  # (B, T, vocab_size)\n",
    "        out = {\"logits\": logits}\n",
    "        if return_hidden:\n",
    "            out[\"hidden\"] = h\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self,\n",
    "             prev_ids: torch.Tensor,     # (B, T_prev)\n",
    "             user_static: Optional[torch.Tensor] = None,\n",
    "             short_term: Optional[torch.Tensor] = None,\n",
    "             long_term: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        out = self.forward(prev_ids, user_static, short_term, long_term, return_hidden=False)\n",
    "        logits_last = out[\"logits\"][:, -1, :]  # (B, vocab)\n",
    "        return logits_last\n",
    "\n",
    "class GBPOTrainer:\n",
    "    \"\"\"\n",
    "    强化学习：\n",
    "      - 支持两阶段训练：监督（CE） + RL（GBPO）\n",
    "      - GBPO Loss 参考论文定义：\n",
    "        L_GBPO = -E[ clip(ratio, 1 - eps, 1 + eps) * A ]\n",
    "      - 其中 ratio = pi_new / pi_old，A 为奖励差分（advantage）\n",
    "    \"\"\"\n",
    "    def __init__(self, model, lambda_rl=0.1, clip_ratio=0.2):\n",
    "        self.model = model\n",
    "        self.lambda_rl = lambda_rl\n",
    "        self.clip_ratio = clip_ratio\n",
    "\n",
    "    def compute_supervised_loss(self, logits, targets, pad_id=0):\n",
    "        \"\"\"\n",
    "        标准交叉熵损失（stage 1）\n",
    "        \"\"\"\n",
    "        B, T, V = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, V),\n",
    "            targets.view(-1),\n",
    "            ignore_index=pad_id\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def compute_gbpo_loss(self,\n",
    "                          new_logits,\n",
    "                          old_logits,\n",
    "                          rewards,\n",
    "                          mask=None):\n",
    "        \"\"\"\n",
    "        GBPO 策略优化损失（stage 2）\n",
    "        参数：\n",
    "          new_logits: 当前策略输出 (B, T, V)\n",
    "          old_logits: 冻结的旧策略输出 (B, T, V)\n",
    "          rewards: 奖励信号 A (B, T)\n",
    "          mask: (B, T)，可选掩码\n",
    "        \"\"\"\n",
    "        # softmax 概率\n",
    "        logp_new = F.log_softmax(new_logits, dim=-1)\n",
    "        logp_old = F.log_softmax(old_logits.detach(), dim=-1)\n",
    "\n",
    "        # 获取 token 对应概率\n",
    "        # 注意：这里假设 reward 只针对目标 token\n",
    "        # 可改为对完整序列求和\n",
    "        probs_new = F.softmax(new_logits, dim=-1).clamp_min(1e-9)\n",
    "        probs_old = F.softmax(old_logits.detach(), dim=-1).clamp_min(1e-9)\n",
    "\n",
    "        pi_bound = torch.where(\n",
    "            rewards.unsqueeze(-1) >= 0,  # advantage为正\n",
    "            torch.max(probs_old, probs_new.detach()),  # 防止正样本过激上升\n",
    "            torch.max(probs_old, 1 - probs_new.detach())  # 防止负样本概率过低\n",
    "        )\n",
    "\n",
    "        ratio = (probs_new / pi_bound).clamp(1e-3, 10)\n",
    "\n",
    "        adv = rewards.unsqueeze(-1)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "        loss_unclipped = -ratio * adv\n",
    "        loss_clipped = -clipped_ratio * adv\n",
    "        loss = torch.max(loss_unclipped, loss_clipped)\n",
    "\n",
    "        if mask is not None:\n",
    "            loss = loss * mask.unsqueeze(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "    def train_step(self,\n",
    "                   batch,\n",
    "                   optimizer,\n",
    "                   use_rl=False,\n",
    "                   old_logits=None,\n",
    "                   rewards=None):\n",
    "        \"\"\"\n",
    "        单步训练：\n",
    "          - use_rl=False：监督训练阶段\n",
    "          - use_rl=True：RL阶段\n",
    "        \"\"\"\n",
    "        target_ids = batch[\"target_ids\"]\n",
    "        user_static = batch.get(\"user_static\", None)\n",
    "        short_term = batch.get(\"short_term\", None)\n",
    "        long_term = batch.get(\"long_term\", None)\n",
    "\n",
    "        out = self.model(target_ids, user_static, short_term, long_term)\n",
    "        logits = out[\"logits\"]\n",
    "\n",
    "        if not use_rl:\n",
    "            loss = self.compute_supervised_loss(logits, target_ids)\n",
    "        else:\n",
    "            assert old_logits is not None and rewards is not None\n",
    "            loss_rl = self.compute_gbpo_loss(logits, old_logits, rewards)\n",
    "            loss_ce = self.compute_supervised_loss(logits, target_ids)\n",
    "            loss = loss_ce + self.lambda_rl * loss_rl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    B = 4\n",
    "    Ns, Ts, Tl = 8, 32, 128\n",
    "    d_ctx_in = 256\n",
    "    vocab = 5000\n",
    "    T_gen = 4  # 生成序列长度（含 BOS）\n",
    "\n",
    "    model = LazyDecoder(\n",
    "        vocab_size=vocab,\n",
    "        d_model=768,\n",
    "        n_layers=4,\n",
    "        n_heads_q=12,\n",
    "        gkv=3,\n",
    "        d_ff=2048,\n",
    "        d_ctx_in=d_ctx_in,\n",
    "        lkv=1,   # KV 层共享\n",
    "        skv=1,   # 1 表示 K=V\n",
    "        attn_drop=0.0,\n",
    "        resid_drop=0.1,\n",
    "    )\n",
    "\n",
    "    # 假数据\n",
    "    user_static = torch.randn(B, Ns, d_ctx_in)\n",
    "    short_term  = torch.randn(B, Ts, d_ctx_in)\n",
    "    long_term   = torch.randn(B, Tl, d_ctx_in)\n",
    "\n",
    "    # 目标 token 序列（BOS 开头）\n",
    "    target_ids = torch.randint(2, vocab, (B, T_gen))\n",
    "    target_ids[:, 0] = model.bos_id\n",
    "\n",
    "    # 前向测试\n",
    "    out = model(target_ids, user_static, short_term, long_term)\n",
    "    print(\"logits shape:\", out[\"logits\"].shape)\n",
    "\n",
    "    # 简单训练示例 \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    trainer = GBPOTrainer(model, lambda_rl=0.2, clip_ratio=0.2)\n",
    "\n",
    "    print(\"\\\\n[Stage 1] Supervised CE Training:\")\n",
    "    for step in range(3):\n",
    "        batch = {\n",
    "            \"target_ids\": target_ids,\n",
    "            \"user_static\": user_static,\n",
    "            \"short_term\": short_term,\n",
    "            \"long_term\": long_term\n",
    "        }\n",
    "        loss = trainer.train_step(batch, optimizer, use_rl=False)\n",
    "        print(f\"step {step}: CE loss = {loss:.4f}\")\n",
    "\n",
    "    print(\"\\\\n[Stage 2] GBPO Reinforcement Fine-tune:\")\n",
    "    with torch.no_grad():\n",
    "        old_logits = model(target_ids, user_static, short_term, long_term)[\"logits\"]\n",
    "    # 模拟用户奖励信号（随机 ±1）\n",
    "    rewards = torch.randint(low=-1, high=2, size=(B, T_gen)).float()\n",
    "\n",
    "    for step in range(3):\n",
    "        batch = {\n",
    "            \"target_ids\": target_ids,\n",
    "            \"user_static\": user_static,\n",
    "            \"short_term\": short_term,\n",
    "            \"long_term\": long_term\n",
    "        }\n",
    "        loss = trainer.train_step(batch, optimizer, use_rl=True,\n",
    "                                  old_logits=old_logits,\n",
    "                                  rewards=rewards)\n",
    "        print(f\"step {step}: RL loss = {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
